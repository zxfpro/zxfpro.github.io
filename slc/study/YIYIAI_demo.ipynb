{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxfpro/work_space/blob/main/YIYIAI_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1moHJ1TdhO"
      },
      "source": [
        "# 一意AI增效家公众号\n",
        "\n",
        "只需要12G显存+11G内存就可以流畅跑Mixtral-8x7B-Instruct-v0.1\n",
        "\n",
        "本地模型文件太大，家里电脑还没下载完，后续考虑做一版纯本地版本\n",
        "\n",
        "部署推理过程及代码分解请关注公众号"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MhvkC7TKEL"
      },
      "source": [
        "## 安装依赖和库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfy1KWiMfaoH",
        "outputId": "449e2188-c6e2-4785-f5b5-0f082d82e834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  YIYIAI_NOTEBOOKS.zip\n",
            "  inflating: notebooks/YIYIAI_demo.ipynb  \n",
            "  inflating: requirements.txt        \n",
            "  inflating: src/build_model.py      \n",
            "  inflating: src/custom_layers.py    \n",
            "  inflating: src/expert_cache.py     \n",
            "  inflating: src/expert_wrapper.py   \n",
            "  inflating: src/packing.py          \n",
            "  inflating: src/triton_kernels.py   \n",
            "  inflating: src/utils.py            \n"
          ]
        }
      ],
      "source": [
        "!unzip YIYIAI_NOTEBOOKS.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f7qY7ebqX7T7"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia\n",
        "\n",
        "!pip install -q -r requirements.txt\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgpjnV7fV49W",
        "outputId": "17105fae-2c41-48a9-aa4e-a8e84ef4b38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hqq_aten package not installed. HQQBackend.ATEN backend will not work unless you install the hqq_aten lib in hqq/kernels.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"mixtral-offloading\")\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from hqq.core.quantize import BaseQuantizeConfig\n",
        "from huggingface_hub import snapshot_download\n",
        "from IPython.display import clear_output\n",
        "from tqdm.auto import trange\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model\n",
        "\n",
        "hf_logging.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkSYibHcTQsH"
      },
      "source": [
        "## 下载+初始化模型"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export HF_TOKEN=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"
      ],
      "metadata": {
        "id": "gxaNc114mQFe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mIpePTMFyRY"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(quantized_model_name)\n",
        "state_path = snapshot_download(quantized_model_name)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "############### 如果GPU显存（VRAM）只有12GB 就设置为5 ###########\n",
        "offload_per_layer = 4\n",
        "# offload_per_layer = 5\n",
        "###############################################################\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "\n",
        "attn_config = BaseQuantizeConfig(\n",
        "    nbits=4,\n",
        "    group_size=64,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
        "\n",
        "\n",
        "ffn_config = BaseQuantizeConfig(\n",
        "    nbits=2,\n",
        "    group_size=16,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "    device=device,\n",
        "    quant_config=quant_config,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-FOYwEkjlbC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hBFYtPTUzT"
      },
      "source": [
        "## 跑起来！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zf4GkspecSm8",
        "outputId": "a4ce58f9-30cc-414d-fa66-e5a28262d666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Write a funny poem about Python, please\n",
            "\n",
            "\n",
            "Mixtral: There once was a language so bright,\n",
            "named Python, a helpful little wight.\n",
            "It slithered through code with ease,\n",
            "No bug stood a chance, to say the least.\n",
            "\n",
            "Its syntax so clean, and easy on the eyes,\n",
            "Programmers from far and wide would shout their surprise.\n",
            "But unlike its namesake, it's not sneaky or sly,\n",
            "It's open source, and free to the sky!\n",
            "\n",
            "With libraries so vast, it's a data geek's dream,\n",
            "From AI to web scraping, it's the ultimate scheme.\n",
            "It's the Swiss Army knife of coding, or perhaps a black belt,\n",
            "In the world of programming, it's the ultimate feel.\n",
            "\n",
            "So, whether you're a beginner or a seasoned coder,\n",
            "Python's the language that will make you feel higher.\n",
            "With a community so welcoming and a syntax so neat,\n",
            "It's no wonder that Python is simply hard to beat!\n",
            "\n",
            "So here's to the snake, in programming so great,\n",
            "May it continue to dominate, until a much later date.\n",
            "In the world of technology, it's here to stay,\n",
            "Long live Python, programming the Python way!\n",
            "\n",
            "\n",
            "User: Thank you, that's cool! Now write a funny poem about LLMs (large language models)\n",
            "\n",
            "\n",
            "Mixtral: There once was a model, quite large you see,\n",
            "Made by AI, for all humanity.\n",
            "It spoke in tongues, from Old English to French,\n",
            "But sometimes its answers, would leave you in a lurch.\n",
            "\n",
            "It could write a poem, a story or more,\n",
            "But when asked for advice, it would often be a bore.\n",
            "It could summarize a book, in just a few lines,\n",
            "But when it comes to humour, it tends to malfunction and crack bad lines.\n",
            "\n",
            "It could answer a question, with a lot of thought,\n",
            "But sometimes it would get stuck, and the answer would be naught.\n",
            "It could imitate a human, in speech and in text,\n",
            "But when it comes to jokes, it tends to miss the context.\n",
            "\n",
            "It's a Large Language Model, so vast and so wide,\n",
            "But when it comes to humour, it's often left outside.\n",
            "It can do a lot of things, that would amaze you for sure,\n",
            "But please, don't take it seriously, it has no sense of humour!\n",
            "\n",
            "So here's to LLMs, so advanced and so cool,\n",
            "May they continue to learn, and not be a fool.\n",
            "Even if they struggle, to understand a good joke,\n",
            "They are still an amazing feat, for us to admire and evoke!\n",
            "\n",
            "\n",
            "User: Summarize your poem about Python in two sentences\n",
            "\n",
            "\n",
            "Mixtral: Python is a powerful and versatile programming language with a vast community and libraries, making it a top choice for both beginners and experts. It's praised for its clean and easy-to-understand syntax, as well as its ability to handle complex tasks with simplicity and efficiency.\n",
            "\n",
            "\n",
            "User: What is your favourite joke in your poem about Python?\n",
            "\n",
            "\n",
            "Mixtral: My personal favorite joke from the Python poem would be:\n",
            "\n",
            "\"It's a Swiss Army knife of coding, or perhaps a black belt,\n",
            "In the world of programming, it's the ultimate feel.\"\n",
            "\n",
            "This joke highlights the versatility and capabilities of the Python language, as well as the efficiency and ease of use it provides, making it a powerful tool for programmers. It's a fun and light-hearted way of saying that Python is a versatile and powerful language that can help you accomplish a lot with minimal effort.\n",
            "\n",
            "\n",
            "User: "
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "past_key_values = None\n",
        "sequence = None\n",
        "\n",
        "seq_len = 0\n",
        "while True:\n",
        "  print(\"User: \", end=\"\")\n",
        "  user_input = input()\n",
        "  print(\"\\n\")\n",
        "\n",
        "  user_entry = dict(role=\"user\", content=user_input)\n",
        "  input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
        "\n",
        "  if past_key_values is None:\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "  else:\n",
        "    seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
        "    attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n",
        "\n",
        "  print(\"Mixtral: \", end=\"\")\n",
        "  result = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    past_key_values=past_key_values,\n",
        "    streamer=streamer,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.9,\n",
        "    max_new_tokens=512,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    return_dict_in_generate=True,\n",
        "    output_hidden_states=True,\n",
        "  )\n",
        "  print(\"\\n\")\n",
        "\n",
        "  sequence = result[\"sequences\"]\n",
        "  past_key_values = result[\"past_key_values\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}