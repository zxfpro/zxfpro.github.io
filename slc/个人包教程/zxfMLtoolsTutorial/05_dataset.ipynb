{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0dcd5cf-095d-4dd2-bca3-bc70a8b3e56e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dataset_home=/Users/zhaoxuefeng/Documents/Project/zxfMLtools_Tutorial\n"
     ]
    }
   ],
   "source": [
    "%env dataset_home=/Users/zhaoxuefeng/Documents/Project/zxfMLtools_Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43285364-ca3e-42c9-9ff5-1dafb5d4ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zxfMLtools.datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c90f3d9-b355-458c-b5ba-30cc46266b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162bb36d-4fdb-4360-bccc-209990bea864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /Users/zhaoxuefeng/GitHub/zxfMLtools/zxfMLtools/datasets.py\n",
    "\"\"\"\n",
    "datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataset 主司 数据集的管理 方便的存储 管理 使用现有的主流数据集 或者自定义数据集\n",
    "应该可以快速的把 文件 零散的表 组装成一个数据集\n",
    "dataset info  制定好合适的下游  适合什么样的模型 什么样的模型不适合\n",
    "dataset evals 什么样的数据是好数据 什么样的数据不是好数据\n",
    "dataset clean 简单来说 就是将坏的数据变成好的数据\n",
    "dataset manager ....\n",
    "\n",
    "\n",
    "\n",
    "# 可完整运行\n",
    "\n",
    "\n",
    "\n",
    "!pip install datasets transformers\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(model.config.id2label[predicted_label])\n",
    "\n",
    "\n",
    "## dataset\n",
    "dataset = tv.datasets.MNIST(\".\", download=True, transform=tv.transforms.ToTensor())\n",
    "train, val = data.random_split(dataset, [55000, 5000])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as torch_data\n",
    "import torchvision\n",
    "import torch\n",
    "from datasets import load_dataset_builder\n",
    "from datasets import Dataset\n",
    "from datasets import IterableDataset\n",
    "from datasets import load_dataset\n",
    "from datasets import get_dataset_config_names\n",
    "from datasets import get_dataset_split_names\n",
    "from datasets import DatasetDict\n",
    "import os\n",
    "from typing import List, Callable\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "class datasethelp():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        maps\n",
    "        iterable\n",
    "        set_transform\n",
    "        env\n",
    "        totorch\n",
    "        save\n",
    "        other\n",
    "\n",
    "        \"\"\"\n",
    "        self.maps = \"\"\"\n",
    "from torchvision import transforms\n",
    "transform = transforms.Resize((224, 224))\n",
    "\n",
    "def maps(example):\n",
    "    example['img'] = transform(example['img'])\n",
    "    return example\n",
    "\n",
    "das = dataset.map(maps)# batched=True\n",
    "\"\"\"\n",
    "        self.iterable = \"\"\"\n",
    "datas.dataset['train'].to_iterable_dataset()\n",
    "\"\"\"\n",
    "        self.set_transform = \"\"\"\n",
    "dataset.set_transform(transforms)     \n",
    "\"\"\"\n",
    "        self.env = \"\"\"dataset_home\"\"\"\n",
    "        self.totorch = \"\"\"ds.with_format(\"torch\", device=device)\"\"\"\n",
    "        self.save = \"\"\"\n",
    ".save_to_disk(\"my_dataset\")\n",
    ".push_to_hub(\"<username>/my_dataset\")\n",
    "        \"\"\"\n",
    "        self.other = \"\"\"\n",
    "import os\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "ds = split_dataset_by_node(ds, rank=int(os.environ[\"RANK\"]), world_size=int(os.environ[\"WORLD_SIZE\"]))  \n",
    "        \"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\"\"\n",
    "        maps\n",
    "        iterable\n",
    "        set_transform\n",
    "        env\n",
    "        totorch\n",
    "        save\n",
    "        other\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "DatasetDict.help = datasethelp()\n",
    "\n",
    "\n",
    "## HELP ##\n",
    "\n",
    "\n",
    "class TDataset(torch_data.Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, target: List = None, pretransform=None, transform=None, window=1,\n",
    "                 type='stepby', info=None):\n",
    "        \"\"\"\n",
    "        data: pd.DataFrame,target:List=None,pretransform=None,transform=None, window=1, type='stepby',info = None):\n",
    "\n",
    "        :param data:\n",
    "        :param target:\n",
    "        :param pretransform:\n",
    "        :param transform:\n",
    "        :param window:\n",
    "        :param type:\n",
    "        :param info:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if pretransform is not None:\n",
    "            data = pretransform.fit_transform(data)\n",
    "        self.data = data\n",
    "        self.window = window\n",
    "        self.type = type\n",
    "        if window == 1:\n",
    "            self.data_list = self.data\n",
    "        else:\n",
    "            self.data_list = self.data  # self.data_list = devide_data(self.data, window, type)\n",
    "        self.transform = transform\n",
    "        if target is not None:\n",
    "            assert len(target) == len(self.data_list)\n",
    "            self.target = pd.DataFrame(target)\n",
    "        else:\n",
    "            cond = [0] * len(self.data_list)\n",
    "            self.target = pd.DataFrame(cond)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.window == 1:\n",
    "            datas = self.data_list.iloc[idx]\n",
    "        else:\n",
    "            datas = self.data_list[idx]\n",
    "        conds = self.target.iloc[idx]\n",
    "        if self.transform is not None:\n",
    "            datas = self.transform(datas)\n",
    "        return datas, conds.values\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    def __new__(cls, *args, mode='use', **kwargs):\n",
    "        dataset_home = os.environ.get('dataset_home', '/qe/data/data/Dataset_LLM')\n",
    "        try:\n",
    "            path = args[0]\n",
    "            if not path.startswith('/'):\n",
    "                path = os.path.join(dataset_home, path)\n",
    "            args = (path,) + args[1:]\n",
    "        except IndexError as e:\n",
    "            path = kwargs['path']\n",
    "            if not path.startswith('/'):\n",
    "                path = os.path.join(dataset_home, path)\n",
    "            kwargs['path'] = path\n",
    "\n",
    "        if mode == 'info':\n",
    "            instance = super().__new__(cls)\n",
    "            return instance\n",
    "        elif mode == 'use':\n",
    "            dataset = load_dataset(*args, **kwargs)\n",
    "            return dataset\n",
    "\n",
    "    def __init__(self, path: str, **kwargs):\n",
    "        \"\"\"\n",
    "        \"totto\"\n",
    "        dataset_home\n",
    "        \"\"\"\n",
    "        self.configs = get_dataset_config_names(path)\n",
    "        self.split_names = get_dataset_split_names(path)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"configs: {self.configs}\" + '\\n' + f\"split_name: {self.split_names}\" + '\\n' + \"torch.Size([1, 32, 32, 3]): batch: w:h:channel\"\n",
    "\n",
    "    @staticmethod\n",
    "    def from_iter(iters):\n",
    "\n",
    "        dataset = IterableDataset.from_generator(iters)\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(dicts):\n",
    "        dataset = Dataset.from_dict(dicts)\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def from_builder(builder):\n",
    "        dataset = load_dataset_builder(builder)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class DataLoaders():\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Vision():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_dimensions(n):\n",
    "        # 开始时，我们假设最佳的宽度和高度都是n\n",
    "        width, height = n, 1\n",
    "        # 我们从1开始，一直到sqrt(n)，寻找能够被n整除的数\n",
    "        for i in range(1, math.isqrt(n) + 1):\n",
    "            if n % i == 0:\n",
    "                # 如果i能被n整除，那么我们就找到了一个更好的宽度和高度\n",
    "                width, height = n // i, i\n",
    "        return width, height\n",
    "\n",
    "    @staticmethod\n",
    "    def _image_grid(imgs, rows, cols):\n",
    "        # assert len(imgs) == rows*cols\n",
    "        w, h = imgs[0].size\n",
    "        grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "        grid_w, grid_h = grid.size\n",
    "        for i, img in enumerate(imgs):\n",
    "            grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "        return grid\n",
    "\n",
    "    @staticmethod\n",
    "    def _tensor_grid(tensors):\n",
    "        # 假设你有一个形状为[9, 3, 32, 32]的张量\n",
    "        # 使用torchvision.utils.make_grid创建一个图像网格\n",
    "        grid = torchvision.utils.make_grid(tensors, nrow=3)\n",
    "        # 转换颜色空间并显示图像\n",
    "        return np.transpose(grid, (1, 2, 0))\n",
    "\n",
    "    @staticmethod\n",
    "    def _show(grid):\n",
    "        plt.imshow(grid)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def run(datas: 'Tensor' or 'data'):\n",
    "        if isinstance(datas, torch.Tensor):\n",
    "            grid = super()._tensor_grid(datas)\n",
    "        else:\n",
    "            imgs = datas['img']\n",
    "            labels = datas['label']\n",
    "            assert len(imgs) == len(labels)\n",
    "            n = len(imgs)\n",
    "            width, height = super()._find_dimensions(n)\n",
    "            grid = super()._image_grid(imgs, rows=height, cols=width)\n",
    "            label_grid = np.array(labels).reshape(height, width)\n",
    "            print(label_grid)\n",
    "        super().show(grid)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datas = Dataset(\"/qe/data/data/Dataset_LLM/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c97c81-6d30-438d-a2fb-181a0ccbab02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a141dfd-b3bb-42ea-a0a6-4ad38a535a23",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /Users/zhaoxuefeng/Documents/Project/zxfMLtools_Tutorial/cifar10/cifar10.py or any data file in the same directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 189\u001b[0m, in \u001b[0;36mDataset.__new__\u001b[0;34m(cls, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 189\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py:2128\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2124\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2125\u001b[0m )\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2128\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py:1814\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1813\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1814\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py:1513\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1515\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /Users/zhaoxuefeng/Documents/Project/zxfMLtools_Tutorial/cifar10/cifar10.py or any data file in the same directory."
     ]
    }
   ],
   "source": [
    "Dataset('cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122aa32-bffe-4016-965b-c880dbbbbe4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Datasets/cifar10',) {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HF google storage unreachable. Downloading and preparing it from source\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset('cifar10')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
