{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a014ff5-f891-4b0d-91b2-9acdd008865b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803813e9-abf4-4e3f-a614-6f683a0c8463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 初始化分布式环境\n",
    "def init_distributed(rank, world_size):\n",
    "    dist.init_process_group(\n",
    "        backend='nccl',  # 如果使用 GPU，则推荐 'nccl'\n",
    "        init_method='env://',  # 使用环境变量来初始化\n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f728e92-d7aa-4139-831a-03038ac7e529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建模型并包装为 DDP 模型\n",
    "def create_ddp_model(model, rank):\n",
    "    model = model.to(rank)  # 将模型移动到对应的设备\n",
    "    ddp_model = DDP(model, device_ids=[rank])  # 包装模型\n",
    "    return ddp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd075e-a333-45d4-9917-28e6fd9cf654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假设您有一个已定义的模型和数据加载器\n",
    "model = ...  # 您的模型定义\n",
    "train_loader = ...  # 您的数据加载器\n",
    "\n",
    "# 设置分布式环境\n",
    "rank = 0  # 当前进程的排名\n",
    "world_size = 4  # 总共的进程数\n",
    "init_distributed(rank, world_size)\n",
    "\n",
    "# 创建 DDP 模型\n",
    "ddp_model = create_ddp_model(model, rank)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in train_loader:\n",
    "        # 正向传播和反向传播\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292051b-d72a-4827-8bd0-cca32932fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ffe19-4035-49d6-93c3-cb6c6da5e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 多机多卡训练\n",
    "分布式训练\n",
    "torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f39ad-7209-447c-ad64-36f0e54cec9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058e45c-f7a7-4cfe-80d6-c9d56c053eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# 初始化分布式环境\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 创建一个简单的数据集\n",
    "class SimpleDataset(Dataset):\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor([index]), torch.tensor([index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "# 创建模型\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def main(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataset = SimpleDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "\n",
    "    # 创建模型并包装为 DDP 模型\n",
    "    model = SimpleModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(2):\n",
    "        for data, target in dataloader:\n",
    "            optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "            outputs = ddp_model(data.to(rank))\n",
    "            loss = torch.nn.functional.mse_loss(outputs, target.to(rank))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 2  # 假设有两个 GPU 可用\n",
    "    for rank in range(world_size):\n",
    "        main(rank, world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c771e6-fc07-477f-a10a-8bf59918c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c137b64-4903-4942-b82e-c0241e93aa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab63e9-4764-4a74-9c3d-42a811c6fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布式训练\n",
    "\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_SOCKET_IFNAME=eth1\n",
    "export NCCL_IB_GID_INDEX=3\n",
    "export NCCL_IB_SL=3\n",
    "export NCCL_NET_GDR_READ=1\n",
    "\n",
    "export MASTER_ADDR=\"${CHIEF_IP:=localhost}\"\n",
    "export MASTER_PORT=\"${MASTER_PORT:=29500}\"\n",
    "\n",
    "path= #path to the project\n",
    "train_path=$path/train/run_clm_lora.py\n",
    "\n",
    "model_path=$path/model/llama2-7B-HF\n",
    "model_save=$path/checkpoint/chinese-llama2-7b-4096-enzh/\n",
    "\n",
    "torchrun --nnodes 1 --node_rank $INDEX --nproc_per_node 8 \\\n",
    "  --master_addr $MASTER_ADDR --master_port $MASTER_PORT  \\\n",
    "  ${train_path} \\\n",
    "  --deepspeed $path/train/deepspeed_config_bf16.json \\\n",
    "  --model_name_or_path ${model_path} \\\n",
    "  --train_file $path/data/instruction/all_instruction_hf.json \\\n",
    "  --validation_file $path/data/instruction/all_instruction_hf_dev.json \\\n",
    "  --preprocessing_num_workers 32 \\\n",
    "  --dataloader_num_workers 16 \\\n",
    "  --dataloader_pin_memory True \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --per_device_eval_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --save_strategy \"steps\" \\\n",
    "  --save_steps 500 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --weight_decay 0. \\\n",
    "  --warmup_ratio 0.03 \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --logging_steps 10 \\\n",
    "  --block_size 4096 \\\n",
    "  --use_lora True \\\n",
    "  --lora_config $path/train/lora_config.json \\\n",
    "  --do_train \\\n",
    "  --bf16 True \\\n",
    "  --bf16_full_eval True \\\n",
    "  --evaluation_strategy \"no\" \\\n",
    "  --validation_split_percentage 0 \\\n",
    "  --streaming \\\n",
    "  --ddp_timeout 72000 \\\n",
    "  --seed 1 \\\n",
    "  --overwrite_output_dir\\\n",
    "  --gradient_checkpointing True \\\n",
    "  --output_dir ${model_save}\n",
    "\n",
    "\n",
    "\n",
    "pip install flash-attn==1.0.4\n",
    "\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_SOCKET_IFNAME=eth1\n",
    "export NCCL_IB_GID_INDEX=3\n",
    "export NCCL_IB_SL=3\n",
    "export NCCL_NET_GDR_READ=1\n",
    "\n",
    "export MASTER_ADDR=\"${CHIEF_IP:=localhost}\"\n",
    "export MASTER_PORT=\"${MASTER_PORT:=29500}\"\n",
    "\n",
    "export HF_HOME=\n",
    "export TRANSFORMERS_CACHE=\n",
    "path= # path to llama2-chinese\n",
    "train_path=$path/train/run_clm_llms_mem.py\n",
    "model_path=$path/model/llama2-7B-HF # place original model here\n",
    "model_save=$path/checkpoint/llama2-7b-llama2_coig_dt_ca-all/\n",
    "\n",
    "# MASTER_ADDR set to localhost\n",
    "HOST_NUM=2\n",
    "torchrun --nnodes $HOST_NUM --node_rank $INDEX --nproc_per_node 8 \\\n",
    "    --master_addr $MASTER_ADDR --master_port $MASTER_PORT  \\\n",
    "    ${train_path} \\\n",
    "    --deepspeed $path/train/deepspeed_config_bf16.json \\\n",
    "    --model_name_or_path ${model_path} \\\n",
    "    --train_file $path/data/instruction/example_instruction_hf.json \\\n",
    "    --validation_file $path/data/instruction/example_instruction_hf_dev.json \\\n",
    "    --preprocessing_num_workers 32 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 500 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 10 \\\n",
    "    --block_size 4096 \\\n",
    "    --do_train \\\n",
    "    --bf16 True \\\n",
    "    --bf16_full_eval True \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --validation_split_percentage 0 \\\n",
    "    --streaming \\\n",
    "    --ddp_timeout 72000 \\\n",
    "    --seed 1 \\\n",
    "    --overwrite_output_dir\\\n",
    "    --gradient_checkpointing True \\\n",
    "    --output_dir ${model_save}\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc91fe-bc36-4aab-914d-3402dff58811",
   "metadata": {},
   "outputs": [],
   "source": [
    "export HCCL_OP_BASE_FFTS_MODE_ENABLE=1\n",
    "export ATB_OPERATION_EXECUTE_ASYNC=1\n",
    "export TASK_QUEUE_ENABLE=1\n",
    "export HCCL_BUFFSIZE=110\n",
    "export ATB_WORKSPACE_MEM_ALLOC_GLOBAL=0\n",
    "export ATB_CONTEXT_WORKSPACE_RING=1\n",
    "export PYTORCH_NPU_ALLOC_CONF=\"max_split_size_mb:2048\"\n",
    "\n",
    "    torchrun --nproc_per_node 2 --master_port 25641 run_llama_parallel_performance.py \\\n",
    "    --load_path \"./llama2-7b-hf_parallel\" \\\n",
    "    --device 2 \\\n",
    "    --batch 1 \\\n",
    "    --seqlen_in 128 \\\n",
    "    --seqlen_out 128 \\\n",
    "    --multi_case 0 \\\n",
    "    --model_name \"LLAMA2-7B\" \\\n",
    "    --multi_batch_size [1] \\\n",
    "    --set_case_pair 0 \\\n",
    "    --seqlen_in_range [5,11] \\\n",
    "    --seqlen_out_range [5,11] \\\n",
    "    --seqlen_in_pair [256,256,512,1024] \\\n",
    "    --seqlen_out_pair [64,256,512,1024]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fceafc5-befa-4b0c-83b0-7e7eac7ed6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 16 02:11:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A30          Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   22C    P0    27W / 165W |   3159MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A30          Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   22C    P0    27W / 165W |   7173MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A30          Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   21C    P0    25W / 165W |      7MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239a2bc-ba58-4c41-ad14-b0b644930277",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.DataParallel  \n",
    "nn.DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996aeab-12fc-4473-84ca-2e4c0e474b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 DistributedDataParallel 更优 效率更高 适应性更好 独立的Python解释器 真正实现分布式训练 适用于单机和多机情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e22ed4-4cc6-46e0-96c7-1c4da73c53f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "# from apex.parallel import DistributedDataParallel as DDP\n",
    "# from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39b12a4-b938-472f-98a4-e8ae583913f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
    "                        help='number of gpus per node')\n",
    "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
    "                        help='ranking within the nodes')\n",
    "    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    args = parser.parse_args()\n",
    "    train(0, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea4776-45bb-448e-afe1-537a2988e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gpu, args):\n",
    "\ttorch.manual_seed(0)\n",
    "    model = ConvNet()\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model.cuda(gpu)\n",
    "    batch_size = 100\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    # Data loading code\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True)\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0 and gpu == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch + 1, \n",
    "                    args.epochs, \n",
    "                    i + 1, \n",
    "                    total_step,\n",
    "                    loss.item())\n",
    "                   )\n",
    "    if gpu == 0:\n",
    "        print(\"Training complete in: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c07424-a76c-4f75-9158-24ec85730437",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235cf13-8c67-4549-a0df-3ec20dac80fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce231ef4-ea86-455a-82b1-772ecaa7ae6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b12281-6210-48a8-aca1-246dc7b12345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eacb2-8b63-4b1e-bd8e-3f138481e00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700c76a8-9bcb-453c-8c3f-566e36c0b984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aa.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile aa.py\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "# from apex.parallel import DistributedDataParallel as DDP\n",
    "# from apex import amp\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--nodes', default=1,type=int, metavar='N')\n",
    "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
    "                        help='number of gpus per node')\n",
    "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
    "                        help='ranking within the nodes')\n",
    "    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    args = parser.parse_args()\n",
    "    #########################################################\n",
    "    args.world_size = args.gpus * args.nodes                #\n",
    "    os.environ['MASTER_ADDR'] = '192.168.8.125'              #\n",
    "    os.environ['MASTER_PORT'] = '8850'                      #\n",
    "    mp.spawn(train, nprocs=args.gpus, args=(args,))         #\n",
    "    #########################################################\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def train(gpu, args):\n",
    "    ############################################################\n",
    "    rank = args.nr * args.gpus + gpu\n",
    "    dist.init_process_group(                                   \n",
    "        backend='nccl',                                         \n",
    "        init_method='env://',                                   \n",
    "        world_size=args.world_size,                              \n",
    "        rank=rank                                               \n",
    "    )                                                          \n",
    "    ############################################################\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    model = ConvNet()\n",
    "    torch.cuda.set_device(gpu)\n",
    "    model.cuda(gpu)\n",
    "    batch_size = 100\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    ###############################################################\n",
    "    # Wrap the model\n",
    "    model = nn.parallel.DistributedDataParallel(model,\n",
    "                                                device_ids=[gpu])\n",
    "    ###############################################################\n",
    "\n",
    "    # Data loading code\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "    ################################################################\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    \ttrain_dataset,\n",
    "    \tnum_replicas=args.world_size,\n",
    "    \trank=rank\n",
    "    )\n",
    "    ################################################################\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False, # shuffle=True,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,sampler=train_sampler)# pin_memory=True)\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0 and gpu == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch + 1, \n",
    "                    args.epochs, \n",
    "                    i + 1, \n",
    "                    total_step,\n",
    "                    loss.item())\n",
    "                   )\n",
    "    if gpu == 0:\n",
    "        print(\"Training complete in: \" + str(datetime.now() - start))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd40d16b-2381-4acd-b6bc-160f2683a354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/upgrade/zxfMLtools_tutl/aa.py\", line 126, in <module>\n",
      "    main()\n",
      "  File \"/home/upgrade/zxfMLtools_tutl/aa.py\", line 50, in main\n",
      "    mp.spawn(train, nprocs=args.gpus, args=(args,))         #\n",
      "  File \"/opt/conda/envs/py310/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 246, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
      "  File \"/opt/conda/envs/py310/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 202, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/opt/conda/envs/py310/lib/python3.10/site-packages/torch/multiprocessing/spawn.py\", line 114, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "  File \"/opt/conda/envs/py310/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/envs/py310/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!/opt/conda/envs/py310/bin/python aa.py -n 1 -g 2 -nr 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9db120-1f7a-4a23-8c29-349e86ee5353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/upgrade/zxfMLtools_tutl\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82ca57-ab34-4f21-ba9e-000489687511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
