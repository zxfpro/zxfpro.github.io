{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aceb7e01-edf2-4b94-b5a8-3608d311310d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import fturning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "920c87ac-9a94-43e0-bc0d-81598885e704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load /Users/zhaoxuefeng/Github/zxfMLtools/zxfMLtools/fturning.py\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from peft import (\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# wget -c\n",
    "\n",
    "\n",
    "class FTModel():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def from_model(self,model, peft_config=None):\n",
    "        \"\"\"\n",
    "        :param model:\n",
    "        :param peft_config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,  # 任务类型 \"CAUSAL_LM\"\n",
    "            inference_mode=False,  # 是否是推理模式\n",
    "            r=8,  # 中间层神经元的个数\n",
    "            lora_alpha=32,  # 一个缩放参数\n",
    "            lora_dropout=0.1  # LoRA层的dropout率\n",
    "        )\n",
    "        peft_config = peft_config or config\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        print(model.print_trainable_parameters())\n",
    "        self.model = model\n",
    "\n",
    "    def train(self,training_args, dataset):\n",
    "        from transformers import Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],  # 训练集\n",
    "            eval_dataset=dataset[\"validation\"],  # 验证集\n",
    "            # data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            #     tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "            # ),\n",
    "        )\n",
    "\n",
    "        # 开始训练和评估模型\n",
    "        trainer.train()\n",
    "        # trainer.train#(resume_from_checkpoint=resume_from_checkpoint)\n",
    "        trainer.evaluate()\n",
    "\n",
    "    def save(self,output_dir):\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class FTModel2():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def load(self,model_name,pretrained=True):\n",
    "        if model_name == 'resnet50':\n",
    "            model = models.resnet50(pretrained=pretrained)\n",
    "        return model\n",
    "\n",
    "    def freeze(self,model):\n",
    "        # 冻结\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def replace_network(self,model,class_names=10):\n",
    "        # 替换最后的全连接层，以适应新的分类任务\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "        return model\n",
    "\n",
    "\n",
    "    def from_torchvision(self,model_name, class_names):\n",
    "        self.model = self.load(model_name=model_name)\n",
    "        self.freeze(self.model)\n",
    "        self.model = self.replace_network(self.model)\n",
    "        return self.model\n",
    "\n",
    "\n",
    "def func1(self, *_, **__):\n",
    "  return get_peft_model_state_dict(self, old_state_dict())\n",
    "model.state_dict = func1.__get__(model, type(model))\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "   model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "    else:\n",
    "        data = load_dataset(data_path)\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "\n",
    "# trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "\n",
    "type(data)\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"], data_point[\"input\"]\n",
    "        )\n",
    "        tokenized_user_prompt = tokenize(\n",
    "            user_prompt, add_eos_token=add_eos_token\n",
    "        )\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [\n",
    "            -100\n",
    "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# %load nlp_weibu/lora_train/lora_train.py\n",
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        # file_name = osp.join(\"templates\", f\"{template_name}.json\")\n",
    "        file_name = \"alpaca.json\"\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Unused imports:\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "\"\"\"\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = \"\",  # the only required argument\n",
    "    data_path: str = \"yahma/alpaca-cleaned\",\n",
    "    output_dir: str = \"./lora-alpaca\",\n",
    "    # training hyperparams\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    val_set_size: int = 2000,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    # wandb params\n",
    "    wandb_project: str = \"\",\n",
    "    wandb_run_name: str = \"\",\n",
    "    wandb_watch: str = \"\",  # options: false | gradients | all\n",
    "    wandb_log_model: str = \"\",  # options: false | true\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    if int(os.environ.get(\"LOCAL_RANK\", 0)) == 0:\n",
    "        print(\n",
    "            f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "            f\"base_model: {base_model}\\n\"\n",
    "            f\"data_path: {data_path}\\n\"\n",
    "            f\"output_dir: {output_dir}\\n\"\n",
    "            f\"batch_size: {batch_size}\\n\"\n",
    "            f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "            f\"num_epochs: {num_epochs}\\n\"\n",
    "            f\"learning_rate: {learning_rate}\\n\"\n",
    "            f\"cutoff_len: {cutoff_len}\\n\"\n",
    "            f\"val_set_size: {val_set_size}\\n\"\n",
    "            f\"lora_r: {lora_r}\\n\"\n",
    "            f\"lora_alpha: {lora_alpha}\\n\"\n",
    "            f\"lora_dropout: {lora_dropout}\\n\"\n",
    "            f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "            f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "            f\"add_eos_token: {add_eos_token}\\n\"\n",
    "            f\"group_by_length: {group_by_length}\\n\"\n",
    "            f\"wandb_project: {wandb_project}\\n\"\n",
    "            f\"wandb_run_name: {wandb_run_name}\\n\"\n",
    "            f\"wandb_watch: {wandb_watch}\\n\"\n",
    "            f\"wandb_log_model: {wandb_log_model}\\n\"\n",
    "            f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "            f\"prompt template: {prompt_template_name}\\n\"\n",
    "        )\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    prompter = Prompter(prompt_template_name)\n",
    "    print(prompter,'prompter')\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "\n",
    "    # Check if parameter passed or if set within environ\n",
    "    use_wandb = len(wandb_project) > 0 or (\n",
    "        \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n",
    "    )\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(wandb_project) > 0:\n",
    "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "    if len(wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = wandb_watch\n",
    "    if len(wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n",
    "\n",
    "    # model = LlamaForCausalLM.from_pretrained(\n",
    "    #     base_model,\n",
    "    #     load_in_8bit=True,\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     device_map=device_map,\n",
    "    # )\n",
    "\n",
    "    # tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "\n",
    "    tokenizer.pad_token_id = (\n",
    "        0  # unk. we want this to be different from the eos token\n",
    "    )\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        # there's probably a way to do this with the tokenizer settings\n",
    "        # but again, gotta move fast\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=add_eos_token\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            if add_eos_token:\n",
    "                user_prompt_len -= 1\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "    else:\n",
    "        data = load_dataset(data_path)\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            resume_from_checkpoint = (\n",
    "                False  # So the trainer won't try loading its state\n",
    "            )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n",
    "\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(\n",
    "            test_size=val_set_size, shuffle=True, seed=42\n",
    "        )\n",
    "        train_data = (\n",
    "            train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "        val_data = (\n",
    "            train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        )\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"wandb\" if use_wandb else None,\n",
    "            run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(\n",
    "            self, old_state_dict()\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\n",
    "        \"\\n If there's a warning about missing keys above, please disregard :)\"\n",
    "    )\n",
    "if __name__ == '__main__':\n",
    "    train(base_model='THUDM/chatglm2-6b',\n",
    "          data_path='yahma/alpaca-cleaned',\n",
    "          output_dir='./lora-alpaca',\n",
    "          batch_size=128,\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!pip\n",
    "install\n",
    "transformers\n",
    "!pip\n",
    "install\n",
    "peft\n",
    "\n",
    "# 导入所需的模块和配置类\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 选择一个预训练的模型和分词器\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "# 设置LoRA的配置参数\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # 任务类型\n",
    "    inference_mode=False,  # 是否是推理模式\n",
    "    r=8,  # 中间层神经元的个数\n",
    "    lora_alpha=32,  # 一个缩放参数\n",
    "    lora_dropout=0.1  # LoRA层的dropout率\n",
    ")\n",
    "\n",
    "# 用预训练的模型初始化一个序列到序列模型类\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 用get_peft_model函数将LoRA方法应用到模型上\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(123)\n",
    "# 打印出可训练的参数数量和比例\n",
    "model.print_trainable_parameters()\n",
    "# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282\n",
    "\n",
    "# 用常规的PyTorch或HuggingFace Trainer API来训练模型\n",
    "# 省略训练代码...\n",
    "\n",
    "\n",
    "Downloading(…)lve / main / config.json: 0 % | | 0.00 / 800[00:00 <?, ?B / s]\n",
    "Downloading\n",
    "pytorch_model.bin: 0 % | | 0.00 / 4.92\n",
    "G[00:00 <?, ?B / s]\n",
    "123\n",
    "trainable\n",
    "params: 2359296 | | all\n",
    "params: 1231940608 | | trainable %: 0.19151053100118282\n",
    "成功\n",
    "\n",
    "# 导入所需的模块\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 选择一个分词器\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "# 加载或创建您的数据集\n",
    "dataset = load_dataset(\"path/to/your/data\")\n",
    "\n",
    "\n",
    "# 或者\n",
    "# dataset = Dataset.from_dict(your_data)\n",
    "\n",
    "# 定义一个分词函数\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"])\n",
    "\n",
    "\n",
    "# 对数据集进行分词\n",
    "dataset = dataset.map(tokenize_dataset)\n",
    "\n",
    "\n",
    "\n",
    "!pip\n",
    "install\n",
    "datasets\n",
    "\n",
    "# 导入所需的模块\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 选择一个分词器\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "\n",
    "# 定义一个分词函数\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"])\n",
    "\n",
    "\n",
    "Downloading(…)okenizer_config.json: 0 % | | 0.00 / 430[00:00 <?, ?B / s]\n",
    "Downloading\n",
    "spiece.model: 0 % | | 0.00 / 4.31\n",
    "M[00:00 <?, ?B / s]\n",
    "Downloading\n",
    "tokenizer.json: 0 % | | 0.00 / 16.3\n",
    "M[00:00 <?, ?B / s]\n",
    "Downloading(…)cial_tokens_map.json: 0 % | | 0.00 / 74.0[00:00 <?, ?B / s]\n",
    "\n",
    "# 导入所需的模块和配置类\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 选择一个预训练的模型和分词器\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "# 设置LoRA的配置参数\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # 任务类型\n",
    "    inference_mode=False,  # 是否是推理模式\n",
    "    r=8,  # 中间层神经元的个数\n",
    "    lora_alpha=32,  # 一个缩放参数\n",
    "    lora_dropout=0.1  # LoRA层的dropout率\n",
    ")\n",
    "\n",
    "# 用预训练的模型初始化一个序列到序列模型类\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 用get_peft_model函数将LoRA方法应用到模型上\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # 输出目录\n",
    "    num_train_epochs=3,  # 训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=16,  # 每个设备上的评估批次大小\n",
    "    warmup_steps=500,  # 热身步数\n",
    "    weight_decay=0.01,  # 权重衰减率\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    logging_steps=10,  # 记录日志的步数间隔\n",
    ")\n",
    "\n",
    "# 加载或创建您的数据集，这里我们直接从网上加载一个文本生成数据集[^2^][2]\n",
    "dataset = load_dataset(\"totto\")\n",
    "\n",
    "\n",
    "# 定义一个分词函数\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"text\"])\n",
    "\n",
    "\n",
    "# 对数据集进行分词\n",
    "dataset = dataset.map(tokenize_dataset)\n",
    "\n",
    "# 创建一个Trainer类实例，并传入模型，训练参数和数据集\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],  # 训练集\n",
    "    eval_dataset=dataset[\"validation\"],  # 验证集\n",
    ")\n",
    "\n",
    "# 开始训练和评估模型\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b763837-984d-438f-8d8b-e304233dbad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e94f5-d7d7-45d6-a966-2ea41c678dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a730f-b81e-4c1c-a777-2fd4985a99ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "WARNING: datasets.builder:Found\n",
    "cached\n",
    "dataset\n",
    "totto( / root /.cache / huggingface / datasets / totto / default / 1.0\n",
    ".0 / 263\n",
    "c85871e5451bc892c65ca0306c0629eb7beb161e0eb998f56231562335dd2)\n",
    "0 % | | 0 / 3[00:00 <?, ?it / s]\n",
    "Map: 0 % | | 0 / 120761[00:00 <?, ? examples / s]\n",
    "╭─────────────────────────────── Traceback(most\n",
    "recent\n",
    "call\n",
    "last) ────────────────────────────────╮\n",
    "│ in < cell\n",
    "line: 46 >:46                                                                            │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / dataset_dict.py: 851 in map                      │\n",
    "│                                                                                                  │\n",
    "│    848 │   │   if cache_file_names is None:                                                      │\n",
    "│    849 │   │   │   cache_file_names = {k: None for k in self}                                    │\n",
    "│    850 │   │   return DatasetDict(                                                               │\n",
    "│ ❱  851 │   │   │   {                                                                             │\n",
    "│    852 │   │   │   │   k: dataset.map(                                                           │\n",
    "│    853 │   │   │   │   │   function = function,                                                    │\n",
    "│    854 │   │   │   │   │   with_indices = with_indices,                                            │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / dataset_dict.py: 852 in < dictcomp >               │\n",
    "│                                                                                                  │\n",
    "│    849 │   │   │   cache_file_names = {k: None for k in self}                                    │\n",
    "│    850 │   │   return DatasetDict(                                                               │\n",
    "│    851 │   │   │   {                                                                             │\n",
    "│ ❱  852 │   │   │   │   k: dataset.map(                                                           │\n",
    "│    853 │   │   │   │   │   function = function,                                                    │\n",
    "│    854 │   │   │   │   │   with_indices = with_indices,                                            │\n",
    "│    855 │   │   │   │   │   with_rank = with_rank,                                                  │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / arrow_dataset.py: 578 in wrapper                 │\n",
    "│                                                                                                  │\n",
    "│    575 │   │ else:                                                                             │\n",
    "│    576 │   │   │   self: \"Dataset\" = kwargs.pop(\"self\")                                          │\n",
    "│    577 │   │  # apply actual function                                                           │\n",
    "│ ❱  578 │   │   out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)                │\n",
    "│    579 │   │   datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [ou  │\n",
    "│    580 │   │   for dataset in datasets:                                                          │\n",
    "│    581 │   │   │  # Remove task templates if a column mapping of the template is no longer val  │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / arrow_dataset.py: 543 in wrapper                 │\n",
    "│                                                                                                  │\n",
    "│    540 │   │   │   \"output_all_columns\": self._output_all_columns, │\n",
    "│    541 │   │}                                                                                 │\n",
    "│    542 │   │  # apply actual function                                                           │\n",
    "│ ❱  543 │   │   out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)                │\n",
    "│    544 │   │   datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [ou  │\n",
    "│    545 │   │  # re-apply format to the output                                                   │\n",
    "│    546 │   │   for dataset in datasets:                                                          │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / arrow_dataset.py: 3073 in map                    │\n",
    "│                                                                                                  │\n",
    "│   3070 │   │   │   │   │   leave = False,                                                          │\n",
    "│   3071 │   │   │   │   │   desc = desc or \"Map\",                                                   │\n",
    "│   3072 │   │   │   │   ) as pbar:                                                                │\n",
    "│ ❱ 3073 │   │   │   │   │   for rank, done, content in Dataset._map_single(**dataset_kwargs):     │\n",
    "│   3074 │   │   │   │   │   │   if done:                                                          │\n",
    "│   3075 │   │   │   │   │   │   │   shards_done += 1                                              │\n",
    "│   3076 │   │   │   │   │   │   │   logger.debug(f\"Finished processing shard number {rank} of {n  │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / arrow_dataset.py: 3427 in _map_single            │\n",
    "│                                                                                                  │\n",
    "│   3424 │   │   │   │   if not batched:                                                           │\n",
    "│   3425 │   │   │   │   │   _time = time.time()                                                   │\n",
    "│   3426 │   │   │   │   │   for i, example in shard_iterable:                                     │\n",
    "│ ❱ 3427 │   │   │   │   │   │   example = apply_function_on_filtered_inputs(example, i, offset=o  │\n",
    "│   3428 │   │   │   │   │   │   if update_data:                                                   │\n",
    "│   3429 │   │   │   │   │   │   │   if i == 0:                                                    │\n",
    "│   3430 │   │   │   │   │   │   │   │   buf_writer, writer, tmp_file = init_buffer_and_writer()   │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / arrow_dataset.py: 3330 in                        │\n",
    "│ apply_function_on_filtered_inputs                                                                │\n",
    "│                                                                                                  │\n",
    "│   3327 │   │   │   │   additional_args += (effective_indices,)                                   │\n",
    "│   3328 │   │   │   if with_rank:                                                                 │\n",
    "│   3329 │   │   │   │   additional_args += (rank,)                                                │\n",
    "│ ❱ 3330 │   │   │   processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)          │\n",
    "│   3331 │   │   │   if isinstance(processed_inputs, LazyDict):                                    │\n",
    "│   3332 │   │   │   │   processed_inputs = {                                                      │\n",
    "│   3333 │   │   │   │   │   k: v\n",
    "for k, v in processed_inputs.data.items() if k not in processed  │\n",
    "│ in tokenize_dataset:43                                                                           │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / datasets / formatting / formatting.py: 270 in __getitem__     │\n",
    "│                                                                                                  │\n",
    "│   267 │   │   return len(self.data)                                                              │\n",
    "│   268 │                                                                                          │\n",
    "│   269 │\n",
    "\n",
    "def __getitem__(self, key):                                                            │\n",
    "\n",
    "│ ❱ 270 │   │   value = self.data[key]                                                             │\n",
    "│   271 │   │   if key in self.keys_to_format:                                                     │\n",
    "│   272 │   │   │   value = self.format(key)                                                       │\n",
    "│   273 │   │   │   self.data[key] = value                                                         │\n",
    "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
    "KeyError: 'text'\n",
    "\n",
    "# final\n",
    "\n",
    "# 导入所需的模块和配置类\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 选择一个预训练的模型和分词器\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "# 设置LoRA的配置参数\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # 任务类型\n",
    "    inference_mode=False,  # 是否是推理模式\n",
    "    r=8,  # 中间层神经元的个数\n",
    "    lora_alpha=32,  # 一个缩放参数\n",
    "    lora_dropout=0.1  # LoRA层的dropout率\n",
    ")\n",
    "\n",
    "# 用预训练的模型初始化一个序列到序列模型类\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 用get_peft_model函数将LoRA方法应用到模型上\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # 输出目录\n",
    "    num_train_epochs=3,  # 训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=16,  # 每个设备上的评估批次大小\n",
    "    warmup_steps=500,  # 热身步数\n",
    "    weight_decay=0.01,  # 权重衰减率\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    logging_steps=10,  # 记录日志的步数间隔\n",
    ")\n",
    "\n",
    "# 加载或创建您的数据集，这里我们直接从网上加载一个文本生成数据集\n",
    "dataset = load_dataset(\"totto\")\n",
    "\n",
    "WARNING: datasets.builder:Found\n",
    "cached\n",
    "dataset\n",
    "totto( / root /.cache / huggingface / datasets / totto / default / 1.0\n",
    ".0 / 263\n",
    "c85871e5451bc892c65ca0306c0629eb7beb161e0eb998f56231562335dd2)\n",
    "0 % | | 0 / 3[00:00 <?, ?it / s]\n",
    "\n",
    "dataset.keys()\n",
    "\n",
    "dict_keys(['train', 'validation', 'test'])\n",
    "\n",
    "dataset['train']\n",
    "\n",
    "Dataset({\n",
    "    features: ['id', 'table_page_title', 'table_webpage_url', 'table_section_title', 'table_section_text', 'table',\n",
    "               'highlighted_cells', 'example_id', 'sentence_annotations', 'overlap_subset'],\n",
    "    num_rows: 120761\n",
    "})\n",
    "\n",
    "\n",
    "# 定义一个分词函数，注意修改键名为\"sentence_annotations\"，以匹配数据集的格式\n",
    "# 并且从字典中选择\"final_sentence\"作为文本数据\n",
    "# def tokenize_dataset(data):\n",
    "#     # print(data,'data')\n",
    "#     # Keys of the returned dictionary will be added to the dataset as columns\n",
    "#     return tokenizer(data[\"sentence_annotations\"][\"final_sentence\"])\n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"sentence_annotations\"][\"final_sentence\"], truncation=True)\n",
    "\n",
    "\n",
    "# 创建一个分词器对象\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "# 定义一个分词函数，注意修改键名为\"final_sentence\"，以匹配数据集的格式\n",
    "# def tokenize_dataset(data):\n",
    "#     # Keys of the returned dictionary will be added to the dataset as columns\n",
    "#     return tokenizer(data[\"sentence_annotations\"])\n",
    "\n",
    "# 对数据集进行分词\n",
    "dataset = dataset.map(tokenize_dataset)\n",
    "\n",
    "# 创建一个Trainer类实例，并传入模型，训练参数和数据集\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],  # 训练集\n",
    "    eval_dataset=dataset[\"validation\"],  # 验证集\n",
    ")\n",
    "\n",
    "# 开始训练和评估模型\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "Map: 0 % | | 0 / 120761[00:00 <?, ? examples / s]\n",
    "Asking\n",
    "to\n",
    "truncate\n",
    "to\n",
    "max_length\n",
    "but\n",
    "no\n",
    "maximum\n",
    "length is provided and the\n",
    "model\n",
    "has\n",
    "no\n",
    "predefined\n",
    "maximum\n",
    "length.Default\n",
    "to\n",
    "no\n",
    "truncation.\n",
    "Map: 0 % | | 0 / 7700[00:00 <?, ? examples / s]\n",
    "Map: 0 % | | 0 / 7700[00:00 <?, ? examples / s]\n",
    "╭─────────────────────────────── Traceback(most\n",
    "recent\n",
    "call\n",
    "last) ────────────────────────────────╮\n",
    "│ in < cell\n",
    "line: 21 >:21                                                                            │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / transformers / trainer.py: 1664 in train                    │\n",
    "│                                                                                                  │\n",
    "│   1661 │   │   inner_training_loop = find_executable_batch_size(                                 │\n",
    "│   1662 │   │   │   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size  │\n",
    "│   1663 │   │   )                                                                                 │\n",
    "│ ❱ 1664 │   │   return inner_training_loop(                                                       │\n",
    "│   1665 │   │   │   args = args,                                                                    │\n",
    "│   1666 │   │   │   resume_from_checkpoint = resume_from_checkpoint,                                │\n",
    "│   1667 │   │   │   trial = trial,                                                                  │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / transformers / trainer.py: 1909 in _inner_training_loop     │\n",
    "│                                                                                                  │\n",
    "│   1906 │   │   │   │   rng_to_sync = True                                                        │\n",
    "│   1907 │   │   │                                                                                 │\n",
    "│   1908 │   │   │   step = -1                                                                     │\n",
    "│ ❱ 1909 │   │   │   for step, inputs in enumerate(epoch_iterator):                                │\n",
    "│   1910 │   │   │   │   total_batched_samples += 1                                                │\n",
    "│   1911 │   │   │   │   if rng_to_sync:                                                           │\n",
    "│   1912 │   │   │   │   │   self._load_rng_state(resume_from_checkpoint)                          │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / torch / utils / data / dataloader.py: 634 in __next__           │\n",
    "│                                                                                                  │\n",
    "│    631 │   │   │   if self._sampler_iter is None:                                                │\n",
    "│    632 │   │   │   │  # TODO(https://github.com/pytorch/pytorch/issues/76750)                   │\n",
    "│    633 │   │   │   │   self._reset()  # type: ignore[call-arg]                                   │\n",
    "│ ❱  634 │   │   │   data = self._next_data()                                                      │\n",
    "│    635 │   │   │   self._num_yielded += 1                                                        │\n",
    "│    636 │   │   │   if self._dataset_kind == _DatasetKind.Iterable and \\                          │\n",
    "│    637 │   │   │   │   │   self._IterableDataset_len_called is not None and  \\                    │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3.10 / dist-packages / torch / utils / data / dataloader.py:678 in _next_data         │\n",
    "│                                                                                                  │\n",
    "│    675 │                                                                                         │\n",
    "│    676 │\n",
    "\n",
    "def _next_data(self):                                                                 │\n",
    "\n",
    "│    677 │   │   index = self._next_index()  # may raise StopIteration                             │\n",
    "│ ❱  678 │   │   data = self._dataset_fetcher.fetch(index)  # may raise StopIteration              │\n",
    "│    679 │   │   if self._pin_memory:                                                              │\n",
    "│    680 │   │   │   data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)            │\n",
    "│    681 │   │   return data                                                                       │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / torch / utils / data / _utils / fetch.py: 54 in fetch             │\n",
    "│                                                                                                  │\n",
    "│   51 │   │   │   │   data = [self.dataset[idx] for idx in possibly_batched_index]                │\n",
    "│   52 │   │ else:                                                                               │\n",
    "│   53 │   │   │   data = self.dataset[possibly_batched_index]                                     │\n",
    "│ ❱ 54 │   │   return self.collate_fn(data)                                                        │\n",
    "│   55                                                                                             │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / transformers / data / data_collator.py: 70 in                 │\n",
    "│ default_data_collator                                                                            │\n",
    "│                                                                                                  │\n",
    "│     67 │  # on the whole batch.                                                                 │\n",
    "│     68 │                                                                                         │\n",
    "│     69 │   if return_tensors == \"pt\":                                                            │\n",
    "│ ❱   70 │   │   return torch_default_data_collator(features)                                      │\n",
    "│     71 │ elif return_tensors == \"tf\":                                                          │\n",
    "│     72 │   │   return tf_default_data_collator(features)                                         │\n",
    "│     73 │ elif return_tensors == \"np\":                                                          │\n",
    "│                                                                                                  │\n",
    "│ / usr / local / lib / python3\n",
    ".10 / dist - packages / transformers / data / data_collator.py: 136 in                │\n",
    "│ torch_default_data_collator                                                                      │\n",
    "│                                                                                                  │\n",
    "│    133 │   │   │ elif isinstance(v, np.ndarray):                                               │\n",
    "│    134 │   │   │   │   batch[k] = torch.tensor(np.stack([f[k] for f in features]))               │\n",
    "│    135 │   │   │ else:                                                                         │\n",
    "│ ❱  136 │   │   │   │   batch[k] = torch.tensor([f[k] for f in features])                         │\n",
    "│    137 │                                                                                         │\n",
    "│    138 │   return batch                                                                          │\n",
    "│    139                                                                                           │\n",
    "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
    "ValueError: expected\n",
    "sequence\n",
    "of\n",
    "length\n",
    "25\n",
    "at\n",
    "dim\n",
    "2(got\n",
    "28)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# final\n",
    "\n",
    "# 导入所需的模块和配置类\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 选择一个预训练的模型和分词器\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "# 设置LoRA的配置参数\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # 任务类型\n",
    "    inference_mode=False,  # 是否是推理模式\n",
    "    r=8,  # 中间层神经元的个数\n",
    "    lora_alpha=32,  # 一个缩放参数\n",
    "    lora_dropout=0.1  # LoRA层的dropout率\n",
    ")\n",
    "\n",
    "# 用预训练的模型初始化一个序列到序列模型类\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 用get_peft_model函数将LoRA方法应用到模型上\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # 输出目录\n",
    "    num_train_epochs=3,  # 训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=16,  # 每个设备上的评估批次大小\n",
    "    warmup_steps=500,  # 热身步数\n",
    "    weight_decay=0.01,  # 权重衰减率\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    logging_steps=10,  # 记录日志的步数间隔\n",
    ")\n",
    "\n",
    "# 加载或创建您的数据集，这里我们直接从网上加载一个文本生成数据集\n",
    "dataset = load_dataset(\"totto\")\n",
    "\n",
    "\n",
    "def tokenize_dataset(data):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"sentence_annotations\"][\"final_sentence\"], truncation=True)\n",
    "\n",
    "\n",
    "# 创建一个分词器对象\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "# 定义一个分词函数，注意修改键名为\"final_sentence\"，以匹配数据集的格式\n",
    "# def tokenize_dataset(data):\n",
    "#     # Keys of the returned dictionary will be added to the dataset as columns\n",
    "#     return tokenizer(data[\"sentence_annotations\"])\n",
    "\n",
    "# 对数据集进行分词\n",
    "dataset = dataset.map(tokenize_dataset)\n",
    "\n",
    "# 创建一个Trainer类实例，并传入模型，训练参数和数据集\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],  # 训练集\n",
    "    eval_dataset=dataset[\"validation\"],  # 验证集\n",
    ")\n",
    "\n",
    "# 开始训练和评估模型\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "ValueError: expected\n",
    "sequence\n",
    "of\n",
    "length\n",
    "25\n",
    "at\n",
    "dim\n",
    "2(got\n",
    "28)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 导入所需的模块和配置类\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 选择一个预训练的模型和分词器\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "\n",
    "# 设置LoRA的配置参数\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # 任务类型\n",
    "    inference_mode=False,  # 是否是推理模式\n",
    "    r=8,  # 中间层神经元的个数\n",
    "    lora_alpha=32,  # 一个缩放参数\n",
    "    lora_dropout=0.1  # LoRA层的dropout率\n",
    ")\n",
    "\n",
    "# 用预训练的模型初始化一个序列到序列模型类\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 用get_peft_model函数将LoRA方法应用到模型上\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # 输出目录\n",
    "    num_train_epochs=3,  # 训练轮数\n",
    "    per_device_train_batch_size=16,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=16,  # 每个设备上的评估批次大小\n",
    "    warmup_steps=500,  # 热身步数\n",
    "    weight_decay=0.01,  # 权重衰减率\n",
    "    logging_dir=\"./logs\",  # 日志目录\n",
    "    logging_steps=10,  # 记录日志的步数间隔\n",
    ")\n",
    "\n",
    "# 加载或创建您的数据集，这里我们直接从网上加载一个文本生成数据集\n",
    "dataset = load_dataset(\"totto\")\n",
    "\n",
    "\n",
    "# 定义一个分词函数，注意修改键名为\"sentence_annotations\"，以匹配数据集的格式\n",
    "# 并且从字典中选择\"final_sentence\"作为文本数据\n",
    "# 并且添加一个参数，将文本截断到模型的最大长度\n",
    "\n",
    "# def tokenize_dataset(data):\n",
    "#     # Keys of the returned dictionary will be added to the dataset as columns\n",
    "#     return tokenizer(data[\"sentence_annotations\"][\"final_sentence\"], truncation=True)\n",
    "\n",
    "def tokenize_dataset(data, tokenizer):\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    return tokenizer(data[\"sentence_annotations\"][\"final_sentence\"], truncation=True)\n",
    "\n",
    "\n",
    "# 创建一个分词器对象，并传入分词函数中作为参数\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "dataset = dataset.map(lambda x: tokenize_dataset(x, tokenizer))\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "# dataset = dataset.map(lambda x: tokenize_dataset(x, tokenizer))\n",
    "\n",
    "# 创建一个Trainer类实例，并传入模型，训练参数和数据集\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],  # 训练集\n",
    "    eval_dataset=dataset[\"validation\"],  # 验证集\n",
    ")\n",
    "\n",
    "# 开始训练和评估模型\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ae3cc-f67e-46f0-92d6-5f34a22fe948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
