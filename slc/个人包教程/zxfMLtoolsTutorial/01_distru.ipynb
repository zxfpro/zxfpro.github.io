{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a014ff5-f891-4b0d-91b2-9acdd008865b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a0c2e-f018-47fc-93bd-cfae7749ac87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacd4f1-a04a-4443-ab81-3553b88b5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义您的模型\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        # 模型结构定义\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播逻辑\n",
    "        ...\n",
    "\n",
    "# 创建模型实例\n",
    "model = YourModel()\n",
    "\n",
    "# 如果有多个 GPU 可用，则使用 DataParallel 包装您的模型\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# 将模型移动到 GPU\n",
    "model.to('cuda')\n",
    "\n",
    "# 创建数据加载器\n",
    "data_loader = DataLoader(your_dataset, batch_size=your_batch_size, shuffle=True)\n",
    "\n",
    "# 训练循环\n",
    "for data in data_loader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.cuda(), labels.cuda()  # 将数据移动到 GPU\n",
    "    outputs = model(inputs)  # 前向传播\n",
    "    loss = loss_function(outputs, labels)  # 计算损失\n",
    "    # 反向传播和优化\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eec9bab-f4ac-4b04-82ae-a542e115674e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How to use Datasets.ipynb'   如何使用分布式训练模型.ipynb\n",
      " aa.py\t\t\t      如何使用单机多卡训练模型.ipynb\n",
      " bb.py\t\t\t      如何微调一个模型.ipynb\n",
      " data\t\t\t      如何训练一个Lora模型.ipynb\n",
      " train_man.py\t\t      如何训练一个模型.ipynb\n",
      " 大模型的使用.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468e6b81-e587-4920-b056-4cdfd38d6ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How to use Datasets.ipynb'   如何使用分布式训练模型.ipynb\n",
      " aa.py\t\t\t      如何使用单机多卡训练模型.ipynb\n",
      " bb.py\t\t\t      如何微调一个模型.ipynb\n",
      " data\t\t\t      如何训练一个Lora模型.ipynb\n",
      " train_man.py\t\t      如何训练一个模型.ipynb\n",
      " 大模型的使用.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a05e5-dd58-4a47-8cf0-503d8d09e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce7507-9d68-4e1c-badc-535ac12a6947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803813e9-abf4-4e3f-a614-6f683a0c8463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# 初始化分布式环境\n",
    "def init_distributed(rank, world_size):\n",
    "    dist.init_process_group(\n",
    "        backend='nccl',  # 如果使用 GPU，则推荐 'nccl'\n",
    "        init_method='env://',  # 使用环境变量来初始化\n",
    "        world_size=world_size,\n",
    "        rank=rank\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f728e92-d7aa-4139-831a-03038ac7e529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建模型并包装为 DDP 模型\n",
    "def create_ddp_model(model, rank):\n",
    "    model = model.to(rank)  # 将模型移动到对应的设备\n",
    "    ddp_model = DDP(model, device_ids=[rank])  # 包装模型\n",
    "    return ddp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd075e-a333-45d4-9917-28e6fd9cf654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 假设您有一个已定义的模型和数据加载器\n",
    "model = ...  # 您的模型定义\n",
    "train_loader = ...  # 您的数据加载器\n",
    "\n",
    "# 设置分布式环境\n",
    "rank = 0  # 当前进程的排名\n",
    "world_size = 4  # 总共的进程数\n",
    "init_distributed(rank, world_size)\n",
    "\n",
    "# 创建 DDP 模型\n",
    "ddp_model = create_ddp_model(model, rank)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    for data, target in train_loader:\n",
    "        # 正向传播和反向传播\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292051b-d72a-4827-8bd0-cca32932fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ffe19-4035-49d6-93c3-cb6c6da5e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 多机多卡训练\n",
    "分布式训练\n",
    "torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f39ad-7209-447c-ad64-36f0e54cec9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058e45c-f7a7-4cfe-80d6-c9d56c053eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# 初始化分布式环境\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 创建一个简单的数据集\n",
    "class SimpleDataset(Dataset):\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor([index]), torch.tensor([index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "# 创建模型\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def main(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    dataset = SimpleDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=10, sampler=sampler)\n",
    "\n",
    "    # 创建模型并包装为 DDP 模型\n",
    "    model = SimpleModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(2):\n",
    "        for data, target in dataloader:\n",
    "            optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.01)\n",
    "            outputs = ddp_model(data.to(rank))\n",
    "            loss = torch.nn.functional.mse_loss(outputs, target.to(rank))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 2  # 假设有两个 GPU 可用\n",
    "    for rank in range(world_size):\n",
    "        main(rank, world_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c771e6-fc07-477f-a10a-8bf59918c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c137b64-4903-4942-b82e-c0241e93aa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab63e9-4764-4a74-9c3d-42a811c6fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布式训练\n",
    "\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_SOCKET_IFNAME=eth1\n",
    "export NCCL_IB_GID_INDEX=3\n",
    "export NCCL_IB_SL=3\n",
    "export NCCL_NET_GDR_READ=1\n",
    "\n",
    "export MASTER_ADDR=\"${CHIEF_IP:=localhost}\"\n",
    "export MASTER_PORT=\"${MASTER_PORT:=29500}\"\n",
    "\n",
    "path= #path to the project\n",
    "train_path=$path/train/run_clm_lora.py\n",
    "\n",
    "model_path=$path/model/llama2-7B-HF\n",
    "model_save=$path/checkpoint/chinese-llama2-7b-4096-enzh/\n",
    "\n",
    "torchrun --nnodes 1 --node_rank $INDEX --nproc_per_node 8 \\\n",
    "  --master_addr $MASTER_ADDR --master_port $MASTER_PORT  \\\n",
    "  ${train_path} \\\n",
    "  --deepspeed $path/train/deepspeed_config_bf16.json \\\n",
    "  --model_name_or_path ${model_path} \\\n",
    "  --train_file $path/data/instruction/all_instruction_hf.json \\\n",
    "  --validation_file $path/data/instruction/all_instruction_hf_dev.json \\\n",
    "  --preprocessing_num_workers 32 \\\n",
    "  --dataloader_num_workers 16 \\\n",
    "  --dataloader_pin_memory True \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --per_device_eval_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --save_strategy \"steps\" \\\n",
    "  --save_steps 500 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --weight_decay 0. \\\n",
    "  --warmup_ratio 0.03 \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --logging_steps 10 \\\n",
    "  --block_size 4096 \\\n",
    "  --use_lora True \\\n",
    "  --lora_config $path/train/lora_config.json \\\n",
    "  --do_train \\\n",
    "  --bf16 True \\\n",
    "  --bf16_full_eval True \\\n",
    "  --evaluation_strategy \"no\" \\\n",
    "  --validation_split_percentage 0 \\\n",
    "  --streaming \\\n",
    "  --ddp_timeout 72000 \\\n",
    "  --seed 1 \\\n",
    "  --overwrite_output_dir\\\n",
    "  --gradient_checkpointing True \\\n",
    "  --output_dir ${model_save}\n",
    "\n",
    "\n",
    "\n",
    "pip install flash-attn==1.0.4\n",
    "\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_SOCKET_IFNAME=eth1\n",
    "export NCCL_IB_GID_INDEX=3\n",
    "export NCCL_IB_SL=3\n",
    "export NCCL_NET_GDR_READ=1\n",
    "\n",
    "export MASTER_ADDR=\"${CHIEF_IP:=localhost}\"\n",
    "export MASTER_PORT=\"${MASTER_PORT:=29500}\"\n",
    "\n",
    "export HF_HOME=\n",
    "export TRANSFORMERS_CACHE=\n",
    "path= # path to llama2-chinese\n",
    "train_path=$path/train/run_clm_llms_mem.py\n",
    "model_path=$path/model/llama2-7B-HF # place original model here\n",
    "model_save=$path/checkpoint/llama2-7b-llama2_coig_dt_ca-all/\n",
    "\n",
    "# MASTER_ADDR set to localhost\n",
    "HOST_NUM=2\n",
    "torchrun --nnodes $HOST_NUM --node_rank $INDEX --nproc_per_node 8 \\\n",
    "    --master_addr $MASTER_ADDR --master_port $MASTER_PORT  \\\n",
    "    ${train_path} \\\n",
    "    --deepspeed $path/train/deepspeed_config_bf16.json \\\n",
    "    --model_name_or_path ${model_path} \\\n",
    "    --train_file $path/data/instruction/example_instruction_hf.json \\\n",
    "    --validation_file $path/data/instruction/example_instruction_hf_dev.json \\\n",
    "    --preprocessing_num_workers 32 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 500 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 10 \\\n",
    "    --block_size 4096 \\\n",
    "    --do_train \\\n",
    "    --bf16 True \\\n",
    "    --bf16_full_eval True \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --validation_split_percentage 0 \\\n",
    "    --streaming \\\n",
    "    --ddp_timeout 72000 \\\n",
    "    --seed 1 \\\n",
    "    --overwrite_output_dir\\\n",
    "    --gradient_checkpointing True \\\n",
    "    --output_dir ${model_save}\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc91fe-bc36-4aab-914d-3402dff58811",
   "metadata": {},
   "outputs": [],
   "source": [
    "export HCCL_OP_BASE_FFTS_MODE_ENABLE=1\n",
    "export ATB_OPERATION_EXECUTE_ASYNC=1\n",
    "export TASK_QUEUE_ENABLE=1\n",
    "export HCCL_BUFFSIZE=110\n",
    "export ATB_WORKSPACE_MEM_ALLOC_GLOBAL=0\n",
    "export ATB_CONTEXT_WORKSPACE_RING=1\n",
    "export PYTORCH_NPU_ALLOC_CONF=\"max_split_size_mb:2048\"\n",
    "\n",
    "    torchrun --nproc_per_node 2 --master_port 25641 run_llama_parallel_performance.py \\\n",
    "    --load_path \"./llama2-7b-hf_parallel\" \\\n",
    "    --device 2 \\\n",
    "    --batch 1 \\\n",
    "    --seqlen_in 128 \\\n",
    "    --seqlen_out 128 \\\n",
    "    --multi_case 0 \\\n",
    "    --model_name \"LLAMA2-7B\" \\\n",
    "    --multi_batch_size [1] \\\n",
    "    --set_case_pair 0 \\\n",
    "    --seqlen_in_range [5,11] \\\n",
    "    --seqlen_out_range [5,11] \\\n",
    "    --seqlen_in_pair [256,256,512,1024] \\\n",
    "    --seqlen_out_pair [64,256,512,1024]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fceafc5-befa-4b0c-83b0-7e7eac7ed6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 16 02:11:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A30          Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   22C    P0    27W / 165W |   3159MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A30          Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   22C    P0    27W / 165W |   7173MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A30          Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   21C    P0    25W / 165W |      7MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239a2bc-ba58-4c41-ad14-b0b644930277",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.DataParallel  \n",
    "nn.DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996aeab-12fc-4473-84ca-2e4c0e474b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 DistributedDataParallel 更优 效率更高 适应性更好 独立的Python解释器 真正实现分布式训练 适用于单机和多机情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "294e63c1-2f99-43e2-8bad-4594adec2443",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://192.168.8.125:8100/simple/\n",
      "Collecting torchvision\n",
      "  Downloading http://192.168.8.125:8100/simple/torchvision/torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.2 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/py310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchvision) (12.3.101)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py310/lib/python3.10/site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py310/lib/python3.10/site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m开始同步\n",
      "\n",
      "同步完毕\n",
      "\n",
      "[\"Looking in indexes: http://192.168.8.125:8100/simple/\\nCollecting torchvision\\n  Downloading http://192.168.8.125:8100/simple/torchvision/torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 116.9 MB/s eta 0:00:00\\nRequirement already satisfied: numpy in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (1.26.2)\\nRequirement already satisfied: requests in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (2.31.0)\\nRequirement already satisfied: torch==2.1.2 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (2.1.2)\\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torchvision) (10.1.0)\\nRequirement already satisfied: filelock in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.13.1)\\nRequirement already satisfied: typing-extensions in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (4.9.0)\\nRequirement already satisfied: sympy in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (1.12)\\nRequirement already satisfied: networkx in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.2.1)\\nRequirement already satisfied: jinja2 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (3.1.2)\\nRequirement already satisfied: fsspec in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2023.12.2)\\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (8.9.2.26)\\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.3.1)\\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (11.0.2.54)\\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (10.3.2.106)\\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (11.4.5.107)\\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.0.106)\\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2.18.1)\\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (12.1.105)\\nRequirement already satisfied: triton==2.1.0 in /opt/conda/envs/py310/lib/python3.10/site-packages (from torch==2.1.2->torchvision) (2.1.0)\\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/py310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchvision) (12.3.101)\\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (3.6)\\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (2.1.0)\\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py310/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py310/lib/python3.10/site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py310/lib/python3.10/site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\\nInstalling collected packages: torchvision\\nSuccessfully installed torchvision-0.16.2\\n\", \"WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\\n\"]\n"
     ]
    }
   ],
   "source": [
    "!pip310 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b12281-6210-48a8-aca1-246dc7b12345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eacb2-8b63-4b1e-bd8e-3f138481e00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216baab7-8237-456c-994b-f3d062fcdd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9db120-1f7a-4a23-8c29-349e86ee5353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/upgrade/zxfMLtools_tutl\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82ca57-ab34-4f21-ba9e-000489687511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
