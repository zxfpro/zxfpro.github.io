{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fcce40-a62b-414d-bdbb-70f87d6d76b0",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca041be-b311-4d57-8b6c-e22ea1dfde72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 如何使用fire 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e25aa-49eb-42ae-a9e2-04ee6088e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "from .prompts import fix\n",
    "import os\n",
    "from zxftools.llms import get_llm\n",
    "\n",
    "def ai_fix(file_path=''):\n",
    "    assert os.path.isfile(file_path)\n",
    "    llm = get_llm(model=os.environ.get('aigen_llm') or 'gpt-3.5-turbo-0613')\n",
    "\n",
    "    with open(file_path,'r') as f:\n",
    "        code = f.read()\n",
    "    result = llm.complete(fix.format(code=code))\n",
    "\n",
    "    with open(file_path,'w') as f:\n",
    "        f.write(result.text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(ai_fix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5984779-dd1b-43ba-aa1f-46e8afe77187",
   "metadata": {},
   "source": [
    "## 编写一个装饰器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2f401-306b-4f9c-8f00-013395173583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def decorator(a = None):\n",
    "    \"\"\"\n",
    "    一个装饰器的编写demo\n",
    "    \"\"\"\n",
    "    def outer_packing(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            result = func(*args, **kwargs)\n",
    "            print(a,'a')\n",
    "            print(func.__name__)  # 函数名\n",
    "            print(args)  # (1, 2)\n",
    "            print(kwargs)  # {'c': 3}\n",
    "            return result\n",
    "        return wrapper\n",
    "    return outer_packing\n",
    "\n",
    "hard_dependencies = ('time',)\n",
    "check_package(hard_dependencies=hard_dependencies,check=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96af8c9-ab85-41f5-9aad-08a0c4a4a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(base_url=\"https://api.bianxieai.com/v1\",\n",
    "                api_key=os.environ.get('bianxieai_API_KEY'))\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"claude-3-5-sonnet-20240620\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "if __name__ == '__main__':\n",
    "    print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31363eec-d92c-440c-9640-80fb5475a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed01dc9-6482-49fb-9d15-7f9069b92869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "class BianXieLLM(CustomLLM):\n",
    "    context_window: int = 3900\n",
    "    num_output: int = 256\n",
    "    model: str = ''\n",
    "    temperature: int = 0.2\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    penalty_score: float = 1.0\n",
    "    stop = []\n",
    "\n",
    "    def __init__(self, temperature=0.2, model='claude-3-5-sonnet-20240620',api_key=None):\n",
    "        super().__init__()\n",
    "        object.__setattr__(self, 'custom_openai_client',OpenAI(base_url=\"https://api.bianxieai.com/v1\",\n",
    "                api_key=api_key or os.environ.get('bianxieai_API_KEY'))\n",
    " )\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        completion = self.custom_openai_client.chat.completions.create(\n",
    "              model=self.model,\n",
    "              temperature = self.temperature,\n",
    "              messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "              **kwargs\n",
    "            )\n",
    "        return CompletionResponse(text=completion.choices[0].message.content)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        completion = self.custom_openai_client.chat.completions.create(\n",
    "              model=self.model,\n",
    "              temperature = self.temperature,\n",
    "              messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "              **kwargs\n",
    "            )\n",
    "        #return CompletionResponse(text=completion.choices[0].message.content)\n",
    "        response_generate = [1,2,3]\n",
    "        response = \"\"\n",
    "        for token in response_generate:\n",
    "            response += token\n",
    "            yield CompletionResponse(text=response, delta=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5392e72-dc1d-4e25-8577-217df84eedf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda39eaf-9b81-43ae-8b2f-667faf980254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e591b-3c78-4180-a807-ecd41d39e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def thread_list(f, params: types.GeneratorType or list, processes=4, chunksize: int = 1):\n",
    "        \"\"\"\n",
    "        1 必须到 if __name__ == '__main__': 下执行\n",
    "        \"\"\"\n",
    "        pool = Pool(processes)\n",
    "        if isinstance(params, types.GeneratorType):\n",
    "            results = pool.imap(f, params, chunksize=chunksize)\n",
    "        else:\n",
    "            results = pool.map(f, params, chunksize=chunksize)\n",
    "        pool.close()\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
