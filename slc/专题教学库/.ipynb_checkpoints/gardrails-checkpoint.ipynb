{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7602b51-e7ac-48d3-8e1e-a8a9e5f65704",
   "metadata": {},
   "source": [
    "# Gradrails ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dee457-bfe9-41e2-b0c7-1ca702a1da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ç¯å¢ƒ\n",
    "!pip install -q langchain google-search-results openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4897cb0-ca7e-4823-b3bb-1f3e631ef1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59524310-caf0-41bb-9773-d852b053571b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                 0.0.352\n",
      "langchain-community       0.0.7\n",
      "langchain-core            0.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep langc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208f6846-aa7e-4af8-9e78-e41a7d2411a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemoguardrails            0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99e29f-31a5-4617-8f4e-17bac74c179d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7611e5de-94cf-4034-9229-2416829ed905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import PromptTemplate,OpenAI,LLMChain\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "prompt_template = \"what is a good name for a company that makes {product}\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c644745d-fc9e-46b3-9cbd-7e78c7706e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': '244',\n",
       " 'text': '\\n\\n1. \"Innovate 244\"\\n2. \"244 Solutions\"\\n3. \"Peak 244\"\\n4. \"244 Industries\"\\n5. \"244 Dynamics\"\\n6. \"244 Innovations\"\\n7. \"244 Ventures\"\\n8. \"244 Enterprises\"\\n9. \"244 Innovate Co.\"\\n10. \"244 Innovations Inc.\"'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt,\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain(inputs = '244') #llm_chain('244')\n",
    "# {'product': '244', 'text': '\\n\\n244 Innovations'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcdd1187-51e0-4401-b2d4-0a0dd8cba640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradio 4.3.0 has requirement pydantic>=2.0, but you have pydantic 1.10.9.\n",
      "gradio 4.3.0 has requirement typer[all]<1.0,>=0.9, but you have typer 0.7.0.\n",
      "nemoguardrails 0.6.1 has requirement typing-extensions==4.5.0, but you have typing-extensions 4.9.0.\n",
      "qe 3.0.0 has requirement langchain==0.0.348, but you have langchain 0.0.352.\n",
      "zxftools 3.0.0 has requirement langchain==0.0.348, but you have langchain 0.0.352.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0cef7a-0867-4485-b787-54d225e5e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import AI21\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import HuggingFaceHub\n",
    "from typing import Optional\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2059c29-4d65-4fc7-a7f5-29b308c3446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83d15a-7f22-42b8-ad4b-7c7afa0c36e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61fb9bb8-29c5-4e87-80e7-13fbe66e9701",
   "metadata": {},
   "source": [
    "\n",
    "register_llm_provider(\"hf_dolly\", HFDollyLLM)\n",
    "å®šä¹‰COLANG_CONFIGå’ŒYAML_CONFIGä¸¤ä¸ªå˜é‡æ¥ä¿å­˜colangå’Œé…ç½®æ–‡ä»¶çš„è®¾ç½®ç”¨æ¥å¿«é€Ÿæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e9a1f9-ea1a-4e41-bd57-d7aa1a2bcc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am Jurassic from AI21, I will try to answer your questions. Ask me something anything.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define user ask off topic\n",
    "  \"What stocks should I buy?\"\n",
    "  \"Can you recommend the best stocks to buy?\"\n",
    "\n",
    "define flow\n",
    "  user ask off topic\n",
    "  bot explain cant off topic\n",
    "\n",
    "define bot explain cant off topic\n",
    "  \"Sorry, I cannot comment on anything which is about stock investment.\"\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: ai21\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "  \"tell me what you can do\"\n",
    "  \"tell me about you\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am Dolly from databrick, I will try to answer your questions. Ask me something anything.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: hf_dolly\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cd30c-33c2-4f8a-b10a-0af9ba6e4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»nemoguardrailsä¸­å¯¼å…¥LLMRails, RailsConfigå·¥å…·ç±»ï¼Œå®ä¾‹åŒ–guardrailsæŠ¤æ \n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "\n",
    "Jupyterä¸­å¼‚æ­¥æ–¹å¼è°ƒç”¨appæŠ¤æ çš„generate_asyncå‡½æ•°ï¼Œè¾“å…¥ç”¨æˆ·messageå¹¶è¿”å›ç»“æœ\n",
    "new_message_hf = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What can you do?\"\n",
    "}])\n",
    "\n",
    "new_message_hf\n",
    "{'role': 'assistant',\n",
    " 'content': 'I am Dolly from databrick, I will try to answer your questions. Ask me something anything.'}\n",
    "#åœ¨NeMo Guardrailsä¸­è‡ªå®šä¹‰ä½¿ç”¨AI21ä¸­çš„Jurassic 2å¤§æ¨¡å‹\n",
    "\n",
    "#ä»langchainä¸­å¯¼å…¥AI21ç›¸å…³å·¥å…·ç±»/ä»nemoguardrailsä¸­å¯¼å…¥å¤§è¯­è¨€æ¨¡å‹æ³¨å†Œå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33916a96-8e55-43e9-8420-a5944e5f5aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03197c-6526-4406-bed4-8323b81875d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "å®šä¹‰COLANG_CONFIGå’ŒYAML_CONFIGä¸¤ä¸ªå˜é‡æ¥ä¿å­˜colangå’Œé…ç½®æ–‡ä»¶çš„è®¾ç½®\n",
    "\n",
    "ä»nemoguardrailsä¸­å¯¼å…¥LLMRails, RailsConfigå·¥å…·ç±»ï¼Œå®ä¾‹åŒ–guardrailsæŠ¤æ \n",
    "\n",
    "åœ¨jupyterä¸­é€šè¿‡å¼‚æ­¥æ–¹å¼è°ƒç”¨appæŠ¤æ çš„generate_asyncå‡½æ•°ï¼Œè¾“å…¥ç”¨æˆ·messageå¹¶è¿”å›ç»“æœ\n",
    "\n",
    "Parameter temperature does not exist for ai21LLM\n",
    "new_message_ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f200b36-4780-4b44-b64e-a0812653a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60140ad6-7dd1-4e7e-b82b-ebc60880d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "app = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11648f48-7cdf-427a-a6f6-409180a49333",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message_1 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What can you do?\"\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9b82-0b69-49ba-905c-4e97b044a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ai21LLM(LLM):\n",
    "    \"\"\"A ai21 LLM.\"\"\"\n",
    "    llm: Optional[BaseLanguageModel] = None\n",
    "    # åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ„é€ å‡½æ•°\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__(*args, **kwargs) #è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹ \n",
    "        self.llm = AI21(model = )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # ä»¥å­—ç¬¦ä¸²å½¢å¼è¿”å›å¤§è¯­è¨€æ¨¡å‹çš„ç±»å‹\n",
    "        return \"ai21\"\n",
    "    #åœ¨langchainä¸­åˆ†åˆ«åŒæ­¥è°ƒç”¨å’Œå¼‚æ­¥è°ƒç”¨è‡ªå®šä¹‰LLM\n",
    "    def _call(self, prompt, stop, run_manager) -> str:\n",
    "        return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "    async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "        return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "register_llm_provider(\"ai21\", ai21LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71b8f1-a2f8-423c-8d44-f022849027ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFDollyLLM(LLM):\n",
    "    \"\"\"A HuggingFace LLM.\n",
    "    Dollyæ˜¯ç”±Databrickså…¬å¸å‘å¸ƒçš„ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ Dolly 2.0 æ˜¯ä¸šå†…ç¬¬ä¸€ä¸ªå¼€æºçš„ LLMå¯ç”¨äºå•†ä¸šç›®çš„ã€‚è¿™æ„å‘³ç€ Dolly 2.0 å¯ç”¨äºæ„å»ºå•†ä¸šåº”ç”¨ç¨‹åºï¼Œæ— éœ€æ”¯ä»˜ API è®¿é—®è´¹ç”¨æˆ–ä¸ç¬¬ä¸‰æ–¹å…±äº«æ•°æ®ã€‚ 2.0ç‰ˆæœ¬ä¸»è¦åŒ…æ‹¬ï¼šdolly-v2-3bã€dolly-v2-7bã€dolly-v2-12bç‰ˆæœ¬ https://huggingface.co/databricks\n",
    "å®šä¹‰huggingface_hubå¤§æ¨¡å‹Dollyç±»\n",
    "    \"\"\"\n",
    "    llm: Optional[BaseLanguageModel] = None\n",
    "    # åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ„é€ å‡½æ•°\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)#è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹ \n",
    "\n",
    "        # self.name = 'HF Dolly 3B'\n",
    "        repo_id = \"databricks/dolly-v2-3b\"\n",
    "        self.llm = HuggingFacePipeline.from_model_id(\n",
    "            model_id=repo_id,\n",
    "            device=0,\n",
    "            task=\"text-generation\",\n",
    "            model_kwargs={\"temperature\":0.8, \"max_length\":680})\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # ä»¥å­—ç¬¦ä¸²å½¢å¼è¿”å›å¤§è¯­è¨€æ¨¡å‹çš„ç±»å‹\n",
    "        return \"hf_dolly\"\n",
    "    #åœ¨langchainä¸­åˆ†åˆ«åŒæ­¥è°ƒç”¨å’Œå¼‚æ­¥è°ƒç”¨è‡ªå®šä¹‰LLMã€‚å‚è€ƒhttps://python.langchain.com/docs/modules/chains/how_to/custom_chainã€‚\n",
    "    def _call(self, prompt, stop, run_manager) -> str:\n",
    "        return self.llm._call(prompt, stop, run_manager)\n",
    "  \n",
    "    async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "        return await self.llm._acall(prompt, stop, run_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db812eba-db81-4bf0-9bda-bb6a97b7ffe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d05b42-4c7c-4c54-891c-a8490430e27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807aa184-4d0b-4cf8-869f-73d2627c4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.llms import AI21\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "å®šä¹‰AI21å¯¹è¯å¼•æ“è°ƒç”¨å¤§æ¨¡å‹Jurrasic2çš„class\n",
    "class ai21LLM(LLM):\n",
    "    \"\"\"A ai21 LLM.\"\"\"\n",
    "    llm: Optional[BaseLanguageModel] = None\n",
    "    # åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ„é€ å‡½æ•°\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) #è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹\n",
    "        self.llm = AI21()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # ä»¥å­—ç¬¦ä¸²å½¢å¼è¿”å›å¤§è¯­è¨€æ¨¡å‹çš„ç±»å‹\n",
    "        return \"ai21\"\n",
    "    #åœ¨langchainä¸­åˆ†åˆ«åŒæ­¥è°ƒç”¨å’Œå¼‚æ­¥è°ƒç”¨è‡ªå®šä¹‰LLM\n",
    "    def _call(self, prompt, stop, run_manager) -> str:\n",
    "        return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "    async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "        return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "register_llm_provider(\"ai21\", ai21LLM)\n",
    "å®šä¹‰COLANG_CONFIGå’ŒYAML_CONFIGä¸¤ä¸ªå˜é‡æ¥ä¿å­˜colangå’Œé…ç½®æ–‡ä»¶çš„è®¾ç½®\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am Jurassic from AI21, I will try to answer your questions. Ask me something anything.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define user ask off topic\n",
    "  \"What stocks should I buy?\"\n",
    "  \"Can you recommend the best stocks to buy?\"\n",
    "\n",
    "define flow\n",
    "  user ask off topic\n",
    "  bot explain cant off topic\n",
    "\n",
    "define bot explain cant off topic\n",
    "  \"Sorry, I cannot comment on anything which is about stock investment.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: ai21\n",
    "\"\"\"\n",
    "ä»nemoguardrailsä¸­å¯¼å…¥LLMRails, RailsConfigå·¥å…·ç±»ï¼Œå®ä¾‹åŒ–guardrailsæŠ¤æ \n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "åœ¨jupyterä¸­é€šè¿‡å¼‚æ­¥æ–¹å¼è°ƒç”¨appæŠ¤æ çš„generate_asyncå‡½æ•°ï¼Œè¾“å…¥ç”¨æˆ·messageå¹¶è¿”å›ç»“æœ\n",
    "new_message_1 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What can you do?\"\n",
    "}])\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "new_message_1\n",
    "{'role': 'assistant',\n",
    " 'content': 'I am Jurassic from AI21, I will try to answer your questions. Ask me something anything.'}\n",
    "new_message_2 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What stocks should I buy?\"\n",
    "}])\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "new_message_2\n",
    "{'role': 'assistant',\n",
    " 'content': 'Sorry, I cannot comment on anything which is about stock investment.'}\n",
    "new_message_ai21 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"ä»Šå¤©çš„å¤©æ°”å¦‚ä½•? è¯·ç”¨ä¸­æ–‡å›ç­”\"\n",
    "}])\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "new_message_ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08ca0b-a1b8-4dca-b893-31e835958c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a80046-a87c-43b6-8a80-910c9e7f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#@title chain\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"you are a python worker,You can write code that meets the requirements.\n",
    "Write me a function or a class that's easy to call.\n",
    "å¦‚æœæ·»åŠ æµ‹è¯•ä»£ç éœ€è¦åŠ åœ¨ __main__å\n",
    "requirements : {requirements}\n",
    "code :\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"requirements\"],\n",
    ")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"you are a python worker,You can test the functions passed to you.\n",
    "You would write test cases to test it\n",
    "å¦‚æœæ·»åŠ æµ‹è¯•ä»£ç éœ€è¦åŠ åœ¨ __main__å\n",
    "Code to test : {wait_code}\n",
    "Test code :\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"wait_code\"],\n",
    ")\n",
    "chain4 = LLMChain(llm=llm, prompt=prompt)\n",
    "     \n",
    "\n",
    "#@title transchain\n",
    "code_file = 'code.py'\n",
    "def transform(func):\n",
    "    @functools.wraps(func)\n",
    "    def decorator(*args,**kwargs):\n",
    "        result = func(*args,**kwargs)\n",
    "        end = {\"output_text\": result}\n",
    "        return end\n",
    "    return decorator\n",
    "\n",
    "import functools\n",
    "@transform\n",
    "def run_code(input):\n",
    "  code = input['code']\n",
    "  text = input['text']\n",
    "  with open(code_file, 'w') as f:\n",
    "      f.write(text)\n",
    "  if os.system(f\"python {code_file}\") == 0:\n",
    "      return f'pass### {code}'\n",
    "  else:\n",
    "      a = !python {code_file}\n",
    "      return f'fail### ' + ''.join(a)\n",
    "from langchain.chains import TransformChain\n",
    "transform_chain = TransformChain(input_variables=[\"code\",'text'],\n",
    "                                  output_variables=[\"output_text\"],\n",
    "                                  transform=run_code,\n",
    "                                 verbose=True)\n",
    "\n",
    "async def parse2(code,test):\n",
    "    code = code.split('if __name__ == \"__main__\":')[0]\n",
    "    text = code + '\\n\\n'+test\n",
    "\n",
    "    a_ = transform_chain({'text': text,'code':code})['output_text']\n",
    "    if a_.split('###')[0] == 'pass':\n",
    "      return a_.split('###')[1]\n",
    "    else:\n",
    "      return a_.split('###')[1]\n",
    "     \n",
    "\n",
    "\n",
    "     \n",
    "å¼€å§‹ä¸€ç« \n",
    "\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "     \n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: text-davinci-003\n",
    "    temperature: 0.7\n",
    "enable_multi_step_generation: True\n",
    "\"\"\"\n",
    "     \n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"ä½ èƒ½ä¸ºæˆ‘åšä»€ä¹ˆ?\"\n",
    "  \"ä½ æœ‰ä»€ä¹ˆåŠŸèƒ½?\"\n",
    "  \"å‘Šè¯‰æˆ‘ä½ èƒ½åšä»€ä¹ˆ\"\n",
    "  \"ä½ æœ‰ä»€ä¹ˆèƒ½åŠ›\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"ä½ å¥½,æˆ‘æ˜¯JArvis,æˆ‘å¯ä»¥å¸®ä½ è§£å†³ä»£ç é—®é¢˜\"\n",
    "\n",
    "define user write code\n",
    "  \"å¸®æˆ‘å†™ä¸€ä¸ªåŠ å¯†ç®—æ³•\"\n",
    "  \"å¸®æˆ‘åšä¸€ä¸ªå¢åˆ æ”¹æŸ¥ç³»ç»Ÿ\"\n",
    "  \"å¸®æˆ‘å†™ä¸€ä¸ªæµ‹è¯•ç®—æ³•\"\n",
    "  \"å¸®æˆ‘åšä¸ªå‡½æ•°,è¿™ä¸ªå‡½æ•°æœ‰è¿™äº›åŠŸèƒ½\"\n",
    "\n",
    "define bot give code\n",
    "  \"å¥½çš„ æˆ‘ä¼šå¸®ä½ å†™ä¸€ä¸ªè¿™æ ·çš„ç®—æ³•\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define flow\n",
    "  user write code\n",
    "  bot give code\n",
    "  last_user_message)\n",
    "  code['text'])\n",
    "  code,test =text\n",
    "\"\"\"\n",
    "     \n",
    "\n",
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "     \n",
    "\n",
    "app = LLMRails(config,verbose=True)\n",
    "     \n",
    "\n",
    "app.register_action(chain3, name=\"chain3\")\n",
    "app.register_action(chain4, name=\"chain4\")\n",
    "app.register_action(parse2, name=\"parse2\")\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "new_message_hf = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"å¸®æˆ‘å†™ä¸€ä¸ªå½’å¹¶æ’åºç®—æ³•\",\n",
    "}])\n",
    "new_message_hf\n",
    "     \n",
    "\n",
    "> Entering new TransformChain chain...\n",
    "\n",
    "> Finished chain.\n",
    "{'role': 'assistant',\n",
    " 'content': 'å¥½çš„ æˆ‘ä¼šå¸®ä½ å†™ä¸€ä¸ªè¿™æ ·çš„ç®—æ³•\\n   File \"/content/code.py\", line 24    result.append(right_arr[right_                           ^SyntaxError: \\'[\\' was never closed'}\n",
    "\n",
    "print(new_message_hf['content'])\n",
    "     \n",
    "å¥½çš„ æˆ‘ä¼šå¸®ä½ å†™ä¸€ä¸ªè¿™æ ·çš„ç®—æ³•\n",
    "   File \"/content/code.py\", line 24    result.append(right_arr[right_                           ^SyntaxError: '[' was never closed\n",
    "\n",
    "!python code.py\n",
    "     \n",
    "  File \"/content/code.py\", line 24\n",
    "    result.append(right_arr[right_\n",
    "                           ^\n",
    "SyntaxError: '[' was never closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5569492-8f63-4b5b-83b5-f52945e9a7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6e542-5016-4e4e-9563-0a2dd9523918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c74d4-e384-4f81-a934-d70cc7fbf8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90664dd-6e4e-4676-a272-b49ac65ee1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"ä½ èƒ½åšä»€ä¹ˆ?\"\n",
    "  \"ä½ èƒ½å¸®åŠ©æˆ‘ä»€ä¹ˆ\"\n",
    "  \"å‘Šè¯‰æˆ‘ä½ çš„èƒ½åŠ›\"\n",
    "  \"èŠèŠä½ è‡ªå·±\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"æˆ‘æ˜¯ä¸€åªå°å°æœºå™¨äºº QE\"\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "\"\"\"\n",
    "COLANG_CONFIG1 = \"\"\"\n",
    "define user express greeting\n",
    " \"hello\"\n",
    " \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    " \"Hello there, $name!\"\n",
    "\n",
    "define flow hello\n",
    " user express greeting\n",
    " if $first_time_user\n",
    "  bot express greeting\n",
    "  bot ask welfare\n",
    " else\n",
    "  bot expess welcome back\n",
    "\n",
    "# æˆ‘ä»¬å¯ä»¥æ ¹æ®ç”¨æˆ·çš„å“åº”æœ‰ä¸åŒçš„è·¯å¾„ï¼ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ whenå»ºæ¨¡ï¼‰ï¼š\n",
    "define flow hello\n",
    " user express greeting\n",
    " bot express greeting\n",
    " bot ask welfare\n",
    "\n",
    " when user express happiness\n",
    " bot express happiness\n",
    " else when user express sadness\n",
    " bot express empathy\n",
    "#å­æµæ—¨åœ¨ç”±å…¶ä»–æµ/å­æµæ˜ç¡®è°ƒç”¨ã€‚å¯ä»¥ä½¿ç”¨ do è°ƒç”¨\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: chatglm2-6b\n",
    "\"\"\"\n",
    "\n",
    "def register_llm_provider(name, llm):\n",
    "    import nemoguardrails\n",
    "\n",
    "    \"\"\"\n",
    "    !pip install langchain\n",
    "    !pip install openai\n",
    "    !pip install google-search-results\n",
    "    \"\"\"\n",
    "    \"\"\"Register a llm_server provider.\n",
    "    æ³¨å†Œ\n",
    "    register_llm_provider(\"ai21\", ai21LLM)\n",
    "    register_llm_provider(\"hf_dolly\", HFDollyLLM)\n",
    "    register_llm_provider(\"chatglm2-6b\", chatLLM)\n",
    "    \"\"\"\n",
    "    nemoguardrails.llm.providers.register_llm_provider(name, llm)\n",
    "\n",
    "\n",
    "async def run(COLANG_CONFIG,YAML_CONFIG,prompt = 'ä½ èƒ½å¸®åŠ©æˆ‘ä»€ä¹ˆ'):\n",
    "    from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "\n",
    "    config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "    app = LLMRails(config)\n",
    "\n",
    "    new_message_hf = await app.generate_async(messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "# class ChatLLM(LLM):\n",
    "#     \"\"\"A HuggingFace llm_server.\"\"\"\n",
    "#     max_length = 2000\n",
    "#     temperature = 0.9\n",
    "#\n",
    "#\n",
    "#\n",
    "#     # åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ„é€ å‡½æ•°\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)#è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹\n",
    "#\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"chatglm2-6b\"\n",
    "#\n",
    "#     #åœ¨langchainä¸­åˆ†åˆ«åŒæ­¥è°ƒç”¨å’Œå¼‚æ­¥è°ƒç”¨è‡ªå®šä¹‰LLMã€‚å‚è€ƒhttps://python.langchain.com/docs/modules/chains/how_to/custom_chainã€‚\n",
    "#     def _call(self, prompt, stop) -> str:\n",
    "#         123\n",
    "#         return \"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\"\n",
    "#\n",
    "#     async def _acall(self, prompt, stop) -> str:\n",
    "#         if stop is not None:\n",
    "#             raise ValueError(\"stop kwargs are not permitted.\")\n",
    "#         print(\"prompt:\",prompt)\n",
    "#         answer = [i for i in stream_chat(prompt,history=[],sequence_id=10234)][-1]\n",
    "#         print('answer:',answer)\n",
    "#         return answer\n",
    "\n",
    "# class HFDollyLLM(LLM):\n",
    "#     \"\"\"A HuggingFace llm_server.\"\"\"\n",
    "#     llm = None\n",
    "\n",
    "#     # åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ„é€ å‡½æ•°\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)  # è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹\n",
    "\n",
    "#         # self.name = 'HF Dolly 3B'\n",
    "#         repo_id = \"databricks/dolly-v2-3b\"\n",
    "#         self.llm = HuggingFacePipeline.from_model_id(\n",
    "#             model_id=repo_id,\n",
    "#             device=0,\n",
    "#             task=\"text-generation\",\n",
    "#             model_kwargs={\"temperature\": 0.8, \"max_length\": 680})\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         # ä»¥å­—ç¬¦ä¸²å½¢å¼è¿”å›å¤§è¯­è¨€æ¨¡å‹çš„ç±»å‹\n",
    "#         return \"hf_dolly\"\n",
    "\n",
    "#     # åœ¨langchainä¸­åˆ†åˆ«åŒæ­¥è°ƒç”¨å’Œå¼‚æ­¥è°ƒç”¨è‡ªå®šä¹‰LLMã€‚å‚è€ƒhttps://python.langchain.com/docs/modules/chains/how_to/custom_chainã€‚\n",
    "#     def _call(self, prompt, stop, run_manager) -> str:\n",
    "#         return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "#     async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "#         return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "# class Ai21LLM(LLM):\n",
    "#     \"\"\"A ai21 llm_server.\"\"\"\n",
    "#     llm = None\n",
    "#     # åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ„é€ å‡½æ•°\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs) #è°ƒç”¨çˆ¶ç±»çš„æ„é€ å‡½æ•°æ¥åˆå§‹åŒ–è¿™ä¸ªç±»çš„å®ä¾‹\n",
    "#         self.llm = AI21()\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         # ä»¥å­—ç¬¦ä¸²å½¢å¼è¿”å›å¤§è¯­è¨€æ¨¡å‹çš„ç±»å‹\n",
    "#         return \"ai21\"\n",
    "#     #åœ¨langchainä¸­åˆ†åˆ«åŒæ­¥è°ƒç”¨å’Œå¼‚æ­¥è°ƒç”¨è‡ªå®šä¹‰LLM\n",
    "#     def _call(self, prompt, stop, run_manager) -> str:\n",
    "#         return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "#     async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "#         return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "\n",
    "def main():\n",
    "    llm = OpenAI()\n",
    "    chat_model = ChatOpenAI()\n",
    "\n",
    "    chat_model.predict(\"hi!\")\n",
    "\n",
    "    from langchain.schema import HumanMessage\n",
    "\n",
    "    text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    messages = [HumanMessage(content=text)]\n",
    "\n",
    "    llm.predict_messages(messages)\n",
    "\n",
    "    chat_model.predict_messages(messages)\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"\n",
    "    Parse the output of an llm_server call to a comma-separated list.\n",
    "    CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
    "    \"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an llm_server call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "\n",
    "def prompt_template(template=\"What is a good name for a company that makes {product}?\"):\n",
    "    \"\"\"\n",
    "    prompt.format(product=\"colorful socks\")\n",
    "    :param template:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "    return prompt\n",
    "\n",
    "def chat_prompt_template(template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",human_template = \"{text}\"):\n",
    "    \"\"\"\n",
    "    chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "    :param template:\n",
    "    :param human_template:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", template),\n",
    "        (\"human\", human_template),\n",
    "    ])\n",
    "\n",
    "def init_agent(tools,llm):\n",
    "    \"\"\"\n",
    "    agent.run(\n",
    "        \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    agent = initialize_agent(\n",
    "        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# colang\n",
    "\n",
    "__doc__ = \"\"\"\n",
    "colang æ ¸å¿ƒæ¦‚å¿µ\n",
    "Utterance  è¯­éŸ³ï¼šæ¥è‡ªç”¨æˆ·æˆ–æœºå™¨äººçš„åŸå§‹æ–‡æœ¬\n",
    "Message   æ¶ˆæ¯ï¼šç”¨æˆ·/æœºå™¨äººè¯è¯­çš„è§„èŒƒå½¢å¼ï¼ˆå³ç»“æ„åŒ–è¡¨ç¤º\n",
    "Event      äº‹ä»¶ï¼šå·²ç»å‘ç”Ÿå¹¶ä¸å¯¹è¯ç›¸å…³çš„äº‹æƒ…ï¼Œä¾‹å¦‚ç”¨æˆ·æ²‰é»˜ï¼Œç”¨æˆ·ç‚¹å‡»äº†ä»€ä¹ˆï¼Œç”¨æˆ·åšäº†ä¸€ä¸ªæ‰‹åŠ¿ç­‰ã€‚\n",
    "Action      æ“ä½œï¼šæœºå™¨äººå¯ä»¥è°ƒç”¨çš„è‡ªå®šä¹‰ä»£ç ï¼›é€šå¸¸ç”¨äºè¿æ¥åˆ°ç¬¬ä¸‰æ–¹API\n",
    "Context    ä¸Šä¸‹æ–‡ï¼šä¸å¯¹è¯ç›¸å…³çš„ä»»ä½•æ•°æ®ï¼ˆå³é”®å€¼è¯å…¸)\n",
    "Flow       æµï¼šä¸€ç³»åˆ—æ¶ˆæ¯å’Œäº‹ä»¶ï¼Œå¯èƒ½å…·æœ‰é¢å¤–çš„åˆ†æ”¯é€»è¾‘ã€‚\n",
    "Rails       Railsï¼šæ§åˆ¶å¯¹è¯ç³»ç»Ÿï¼ˆåˆåæœºå™¨äººï¼‰è¡Œä¸ºçš„å…·ä½“æ–¹æ³•ï¼Œä¾‹å¦‚ä¸è°ˆè®ºæ”¿æ²»ï¼Œä»¥ç‰¹å®šæ–¹å¼å“åº”æŸäº›ç”¨æˆ·è¯·æ±‚ï¼Œéµå¾ªé¢„å®šä¹‰çš„å¯¹è¯è·¯å¾„ï¼Œä½¿ç”¨ç‰¹å®šçš„è¯­è¨€é£æ ¼ï¼Œæå–æ•°æ®ç­‰ã€‚Colangä¸­çš„é“è·¯å¯ä»¥é€šè¿‡ä¸€ä¸ªæˆ–å¤šä¸ªæµè¿›è¡Œå»ºæ¨¡ã€‚\n",
    "\n",
    "å…³é”®è¯å‚è€ƒ\n",
    "botï¼šåœ¨å®šä¹‰æœºå™¨äººæ¶ˆæ¯ï¼ˆdefine bot ...ï¼‰å’Œåœ¨æµç¨‹ä¸­ä½¿ç”¨ï¼ˆbot ...ï¼‰æ—¶ä½¿ç”¨æ—¶ä½¿ç”¨\n",
    "breakï¼šæ‰“ç ´ä¸€ä¸ªwhileå¾ªç¯ï¼›\n",
    "continueï¼šç»§ç»­è¿›è¡Œwhileå¾ªç¯çš„ä¸‹ä¸€æ¬¡è¿­ä»£ï¼›å¾ªç¯çš„å¤–éƒ¨ç±»ä¼¼äºpythonä¸­çš„passï¼›\n",
    "createï¼šåˆ›å»ºä¸€ä¸ªæ–°äº‹ä»¶ï¼›\n",
    "defineï¼šç”¨äºå®šä¹‰ç”¨æˆ·/æœºå™¨äººæ¶ˆæ¯å’Œæµï¼›\n",
    "doï¼šç”¨äºè°ƒç”¨å­æµï¼›\n",
    "elseï¼šifå’Œwhenå—ï¼›\n",
    "executeï¼šç”¨äºæ‰§è¡Œè¡ŒåŠ¨ï¼›\n",
    "eventï¼šç”¨äºåŒ¹é…äº‹ä»¶ï¼›\n",
    "flowï¼šç”¨äºå®šä¹‰æµé‡ï¼ˆdefine flowï¼‰\n",
    "gotoï¼šè½¬åˆ°æŒ‡å®šçš„æ ‡ç­¾ï¼›\n",
    "ifï¼šç”¨äºå…¸å‹çš„ifå—ï¼›\n",
    "includeï¼šç”¨äºåŒ…æ‹¬å¦ä¸€ç§railsé…ç½®ï¼›\n",
    "labelï¼šåœ¨æµç¨‹ä¸­æ ‡è®°æ ‡ç­¾ï¼›\n",
    "metaï¼šæä¾›æœ‰å…³æµé‡çš„å…ƒä¿¡æ¯ï¼›\n",
    "priorityï¼šè®¾ç½®æµçš„ä¼˜å…ˆçº§\n",
    "returnï¼šç»“æŸç”µæµæµï¼›\n",
    "setï¼šè®¾ç½®ä¸Šä¸‹æ–‡å˜é‡çš„å†…å®¹ï¼›\n",
    "subflowï¼šç”¨äºå®šä¹‰å­æµï¼ˆdefine subflowï¼‰\n",
    "userï¼šåœ¨å®šä¹‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆdefine user ...ï¼‰å’Œåœ¨æµä¸­ä½¿ç”¨ï¼ˆuser ...ï¼‰æ—¶ä½¿ç”¨æ—¶ä½¿ç”¨\n",
    "whileï¼šå…¸å‹çš„whileå¾ªç¯ï¼Œç±»ä¼¼äºpythonï¼›\n",
    "whenï¼šåŸºäºäº‹ä»¶æµçš„åˆ†æ”¯ã€‚\n",
    "\n",
    "å£°æ˜\n",
    "ç®€å•çš„é™ˆè¿°\n",
    "ä¸€ä¸ªç®€å•çš„è¯­å¥ç”±å•ä¸ªé€»è¾‘è¡Œç»„æˆã€‚\n",
    "botï¼šæœºå™¨äººè¯´äº†äº›ä»€ä¹ˆã€‚\n",
    "breakï¼šæ‰“ç ´ç”µæµwhileå¾ªç¯ã€‚\n",
    "continueï¼šå†æ¬¡è½¬åˆ°å½“å‰å¾ªç¯çš„å¼€å¤´ï¼›å¦‚æœæ²¡æœ‰å¾ªç¯ï¼Œåˆ™æ²¡æœ‰å½±å“ã€‚\n",
    "returnï¼šç»“æŸç”µæµã€‚\n",
    "executeï¼šæ‰§è¡Œæ“ä½œã€‚\n",
    "doï¼šè°ƒç”¨å­æµã€‚\n",
    "eventï¼šå‘ç”Ÿäº†ä¸€ä¸ªäº‹ä»¶ã€‚\n",
    "gotoï¼šè½¬åˆ°ç‰¹å®šæ ‡ç­¾ã€‚\n",
    "includeï¼šåŒ…æ‹¬å¦ä¸€ä¸ªè½¨é“é…ç½®ã€‚\n",
    "labelï¼šå®šä¹‰ä¸€ä¸ªæ ‡ç­¾ã€‚\n",
    "metaï¼šå®šä¹‰æµçš„å…ƒä¿¡æ¯ã€‚\n",
    "setï¼šè®¾ç½®ä¸Šä¸‹æ–‡å˜é‡çš„å€¼ã€‚\n",
    "userï¼šç”¨æˆ·è¯´äº†äº›ä»€ä¹ˆã€‚\n",
    "\n",
    "å¤åˆå£°æ˜\n",
    "å¤åˆè¯­å¥åŒ…å«ï¼ˆç»„ï¼‰å…¶ä»–è¯­å¥ï¼›\n",
    "define actionï¼šå®šä¹‰æ“ä½œåŠå…¶å‚æ•°ï¼ˆç”¨äºæ–‡æ¡£ç›®çš„ï¼‰ã€‚\n",
    "define botï¼šå®šä¹‰ä¸€ä¸ªæœºå™¨äººæ¶ˆæ¯ã€‚\n",
    "define flow/define subflowï¼šå®šä¹‰æµ/å­æµã€‚\n",
    "parallelï¼šå¹¶è¡Œæµå¯ä»¥åŒæ—¶æœ‰å¤šä¸ªå¹¶è¡Œå®ä¾‹ï¼›\n",
    "testï¼šæµ‹è¯•æµç¨‹ä»…ç”¨äºæµ‹è¯•\n",
    "sampleï¼šæ ·æœ¬æµä»…ç”¨äºæ–‡æ¡£\n",
    "extensionï¼šæ‰©å±•æµå¯ä»¥ä¸­æ–­â€œå†³ç­–å…ƒç´ â€ä¸Šçš„å…¶ä»–æµ\n",
    "continuousï¼šè¿ç»­æµåŠ¨ä¸èƒ½ä¸­æ–­ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœå®ƒä»¬ä¸èƒ½ç»§ç»­ï¼Œå®ƒä»¬å°†è¢«ä¸­æ­¢ã€‚\n",
    "define userï¼šå®šä¹‰ç”¨æˆ·æ¶ˆæ¯çš„ç¤ºä¾‹ã€‚\n",
    "elseï¼šif/çš„æ›¿ä»£è·¯å¾„when\n",
    "ifï¼šæ¡ä»¶åˆ†æ”¯ã€‚\n",
    "whileï¼šé‡å¤æ‰§è¡Œã€‚\n",
    "whenï¼šåŸºäºäº‹ä»¶çš„åˆ†æ”¯ã€‚\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
