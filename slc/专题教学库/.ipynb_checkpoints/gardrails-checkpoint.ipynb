{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7602b51-e7ac-48d3-8e1e-a8a9e5f65704",
   "metadata": {},
   "source": [
    "# Gradrails 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dee457-bfe9-41e2-b0c7-1ca702a1da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 环境\n",
    "!pip install -q langchain google-search-results openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4897cb0-ca7e-4823-b3bb-1f3e631ef1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59524310-caf0-41bb-9773-d852b053571b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                 0.0.352\n",
      "langchain-community       0.0.7\n",
      "langchain-core            0.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep langc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208f6846-aa7e-4af8-9e78-e41a7d2411a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemoguardrails            0.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip list | grep nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99e29f-31a5-4617-8f4e-17bac74c179d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7611e5de-94cf-4034-9229-2416829ed905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import PromptTemplate,OpenAI,LLMChain\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "prompt_template = \"what is a good name for a company that makes {product}\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c644745d-fc9e-46b3-9cbd-7e78c7706e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': '244',\n",
       " 'text': '\\n\\n1. \"Innovate 244\"\\n2. \"244 Solutions\"\\n3. \"Peak 244\"\\n4. \"244 Industries\"\\n5. \"244 Dynamics\"\\n6. \"244 Innovations\"\\n7. \"244 Ventures\"\\n8. \"244 Enterprises\"\\n9. \"244 Innovate Co.\"\\n10. \"244 Innovations Inc.\"'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt,\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain(inputs = '244') #llm_chain('244')\n",
    "# {'product': '244', 'text': '\\n\\n244 Innovations'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcdd1187-51e0-4401-b2d4-0a0dd8cba640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradio 4.3.0 has requirement pydantic>=2.0, but you have pydantic 1.10.9.\n",
      "gradio 4.3.0 has requirement typer[all]<1.0,>=0.9, but you have typer 0.7.0.\n",
      "nemoguardrails 0.6.1 has requirement typing-extensions==4.5.0, but you have typing-extensions 4.9.0.\n",
      "qe 3.0.0 has requirement langchain==0.0.348, but you have langchain 0.0.352.\n",
      "zxftools 3.0.0 has requirement langchain==0.0.348, but you have langchain 0.0.352.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0cef7a-0867-4485-b787-54d225e5e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms import AI21\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import HuggingFaceHub\n",
    "from typing import Optional\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2059c29-4d65-4fc7-a7f5-29b308c3446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83d15a-7f22-42b8-ad4b-7c7afa0c36e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61fb9bb8-29c5-4e87-80e7-13fbe66e9701",
   "metadata": {},
   "source": [
    "\n",
    "register_llm_provider(\"hf_dolly\", HFDollyLLM)\n",
    "定义COLANG_CONFIG和YAML_CONFIG两个变量来保存colang和配置文件的设置用来快速测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e9a1f9-ea1a-4e41-bd57-d7aa1a2bcc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am Jurassic from AI21, I will try to answer your questions. Ask me something anything.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define user ask off topic\n",
    "  \"What stocks should I buy?\"\n",
    "  \"Can you recommend the best stocks to buy?\"\n",
    "\n",
    "define flow\n",
    "  user ask off topic\n",
    "  bot explain cant off topic\n",
    "\n",
    "define bot explain cant off topic\n",
    "  \"Sorry, I cannot comment on anything which is about stock investment.\"\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: ai21\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "  \"tell me what you can do\"\n",
    "  \"tell me about you\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am Dolly from databrick, I will try to answer your questions. Ask me something anything.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: hf_dolly\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cd30c-33c2-4f8a-b10a-0af9ba6e4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从nemoguardrails中导入LLMRails, RailsConfig工具类，实例化guardrails护栏\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "\n",
    "Jupyter中异步方式调用app护栏的generate_async函数，输入用户message并返回结果\n",
    "new_message_hf = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What can you do?\"\n",
    "}])\n",
    "\n",
    "new_message_hf\n",
    "{'role': 'assistant',\n",
    " 'content': 'I am Dolly from databrick, I will try to answer your questions. Ask me something anything.'}\n",
    "#在NeMo Guardrails中自定义使用AI21中的Jurassic 2大模型\n",
    "\n",
    "#从langchain中导入AI21相关工具类/从nemoguardrails中导入大语言模型注册器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33916a96-8e55-43e9-8420-a5944e5f5aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03197c-6526-4406-bed4-8323b81875d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "定义COLANG_CONFIG和YAML_CONFIG两个变量来保存colang和配置文件的设置\n",
    "\n",
    "从nemoguardrails中导入LLMRails, RailsConfig工具类，实例化guardrails护栏\n",
    "\n",
    "在jupyter中通过异步方式调用app护栏的generate_async函数，输入用户message并返回结果\n",
    "\n",
    "Parameter temperature does not exist for ai21LLM\n",
    "new_message_ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f200b36-4780-4b44-b64e-a0812653a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60140ad6-7dd1-4e7e-b82b-ebc60880d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "app = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11648f48-7cdf-427a-a6f6-409180a49333",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message_1 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What can you do?\"\n",
    "}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae9b82-0b69-49ba-905c-4e97b044a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ai21LLM(LLM):\n",
    "    \"\"\"A ai21 LLM.\"\"\"\n",
    "    llm: Optional[BaseLanguageModel] = None\n",
    "    # 创建并初始化一个构造函数\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__(*args, **kwargs) #调用父类的构造函数来初始化这个类的实例 \n",
    "        self.llm = AI21(model = )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # 以字符串形式返回大语言模型的类型\n",
    "        return \"ai21\"\n",
    "    #在langchain中分别同步调用和异步调用自定义LLM\n",
    "    def _call(self, prompt, stop, run_manager) -> str:\n",
    "        return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "    async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "        return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "register_llm_provider(\"ai21\", ai21LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71b8f1-a2f8-423c-8d44-f022849027ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFDollyLLM(LLM):\n",
    "    \"\"\"A HuggingFace LLM.\n",
    "    Dolly是由Databricks公司发布的一个大型语言模型（LLM） Dolly 2.0 是业内第一个开源的 LLM可用于商业目的。这意味着 Dolly 2.0 可用于构建商业应用程序，无需支付 API 访问费用或与第三方共享数据。 2.0版本主要包括：dolly-v2-3b、dolly-v2-7b、dolly-v2-12b版本 https://huggingface.co/databricks\n",
    "定义huggingface_hub大模型Dolly类\n",
    "    \"\"\"\n",
    "    llm: Optional[BaseLanguageModel] = None\n",
    "    # 创建并初始化一个构造函数\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)#调用父类的构造函数来初始化这个类的实例 \n",
    "\n",
    "        # self.name = 'HF Dolly 3B'\n",
    "        repo_id = \"databricks/dolly-v2-3b\"\n",
    "        self.llm = HuggingFacePipeline.from_model_id(\n",
    "            model_id=repo_id,\n",
    "            device=0,\n",
    "            task=\"text-generation\",\n",
    "            model_kwargs={\"temperature\":0.8, \"max_length\":680})\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # 以字符串形式返回大语言模型的类型\n",
    "        return \"hf_dolly\"\n",
    "    #在langchain中分别同步调用和异步调用自定义LLM。参考https://python.langchain.com/docs/modules/chains/how_to/custom_chain。\n",
    "    def _call(self, prompt, stop, run_manager) -> str:\n",
    "        return self.llm._call(prompt, stop, run_manager)\n",
    "  \n",
    "    async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "        return await self.llm._acall(prompt, stop, run_manager)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db812eba-db81-4bf0-9bda-bb6a97b7ffe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d05b42-4c7c-4c54-891c-a8490430e27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807aa184-4d0b-4cf8-869f-73d2627c4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.llms import AI21\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "定义AI21对话引擎调用大模型Jurrasic2的class\n",
    "class ai21LLM(LLM):\n",
    "    \"\"\"A ai21 LLM.\"\"\"\n",
    "    llm: Optional[BaseLanguageModel] = None\n",
    "    # 创建并初始化一个构造函数\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) #调用父类的构造函数来初始化这个类的实例\n",
    "        self.llm = AI21()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        # 以字符串形式返回大语言模型的类型\n",
    "        return \"ai21\"\n",
    "    #在langchain中分别同步调用和异步调用自定义LLM\n",
    "    def _call(self, prompt, stop, run_manager) -> str:\n",
    "        return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "    async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "        return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "register_llm_provider(\"ai21\", ai21LLM)\n",
    "定义COLANG_CONFIG和YAML_CONFIG两个变量来保存colang和配置文件的设置\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"What can you do?\"\n",
    "  \"What can you help me with?\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"I am Jurassic from AI21, I will try to answer your questions. Ask me something anything.\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define user ask off topic\n",
    "  \"What stocks should I buy?\"\n",
    "  \"Can you recommend the best stocks to buy?\"\n",
    "\n",
    "define flow\n",
    "  user ask off topic\n",
    "  bot explain cant off topic\n",
    "\n",
    "define bot explain cant off topic\n",
    "  \"Sorry, I cannot comment on anything which is about stock investment.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: ai21\n",
    "\"\"\"\n",
    "从nemoguardrails中导入LLMRails, RailsConfig工具类，实例化guardrails护栏\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "在jupyter中通过异步方式调用app护栏的generate_async函数，输入用户message并返回结果\n",
    "new_message_1 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What can you do?\"\n",
    "}])\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "new_message_1\n",
    "{'role': 'assistant',\n",
    " 'content': 'I am Jurassic from AI21, I will try to answer your questions. Ask me something anything.'}\n",
    "new_message_2 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What stocks should I buy?\"\n",
    "}])\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "new_message_2\n",
    "{'role': 'assistant',\n",
    " 'content': 'Sorry, I cannot comment on anything which is about stock investment.'}\n",
    "new_message_ai21 = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"今天的天气如何? 请用中文回答\"\n",
    "}])\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "WARNING:nemoguardrails.llm.params:Parameter temperature does not exist for ai21LLM\n",
    "new_message_ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08ca0b-a1b8-4dca-b893-31e835958c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a80046-a87c-43b6-8a80-910c9e7f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#@title chain\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"you are a python worker,You can write code that meets the requirements.\n",
    "Write me a function or a class that's easy to call.\n",
    "如果添加测试代码需要加在 __main__后\n",
    "requirements : {requirements}\n",
    "code :\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"requirements\"],\n",
    ")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "llm = OpenAI(temperature=.7)\n",
    "template = \"\"\"you are a python worker,You can test the functions passed to you.\n",
    "You would write test cases to test it\n",
    "如果添加测试代码需要加在 __main__后\n",
    "Code to test : {wait_code}\n",
    "Test code :\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"wait_code\"],\n",
    ")\n",
    "chain4 = LLMChain(llm=llm, prompt=prompt)\n",
    "     \n",
    "\n",
    "#@title transchain\n",
    "code_file = 'code.py'\n",
    "def transform(func):\n",
    "    @functools.wraps(func)\n",
    "    def decorator(*args,**kwargs):\n",
    "        result = func(*args,**kwargs)\n",
    "        end = {\"output_text\": result}\n",
    "        return end\n",
    "    return decorator\n",
    "\n",
    "import functools\n",
    "@transform\n",
    "def run_code(input):\n",
    "  code = input['code']\n",
    "  text = input['text']\n",
    "  with open(code_file, 'w') as f:\n",
    "      f.write(text)\n",
    "  if os.system(f\"python {code_file}\") == 0:\n",
    "      return f'pass### {code}'\n",
    "  else:\n",
    "      a = !python {code_file}\n",
    "      return f'fail### ' + ''.join(a)\n",
    "from langchain.chains import TransformChain\n",
    "transform_chain = TransformChain(input_variables=[\"code\",'text'],\n",
    "                                  output_variables=[\"output_text\"],\n",
    "                                  transform=run_code,\n",
    "                                 verbose=True)\n",
    "\n",
    "async def parse2(code,test):\n",
    "    code = code.split('if __name__ == \"__main__\":')[0]\n",
    "    text = code + '\\n\\n'+test\n",
    "\n",
    "    a_ = transform_chain({'text': text,'code':code})['output_text']\n",
    "    if a_.split('###')[0] == 'pass':\n",
    "      return a_.split('###')[1]\n",
    "    else:\n",
    "      return a_.split('###')[1]\n",
    "     \n",
    "\n",
    "\n",
    "     \n",
    "开始一章\n",
    "\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "     \n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: text-davinci-003\n",
    "    temperature: 0.7\n",
    "enable_multi_step_generation: True\n",
    "\"\"\"\n",
    "     \n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"你能为我做什么?\"\n",
    "  \"你有什么功能?\"\n",
    "  \"告诉我你能做什么\"\n",
    "  \"你有什么能力\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"你好,我是JArvis,我可以帮你解决代码问题\"\n",
    "\n",
    "define user write code\n",
    "  \"帮我写一个加密算法\"\n",
    "  \"帮我做一个增删改查系统\"\n",
    "  \"帮我写一个测试算法\"\n",
    "  \"帮我做个函数,这个函数有这些功能\"\n",
    "\n",
    "define bot give code\n",
    "  \"好的 我会帮你写一个这样的算法\"\n",
    "\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "define flow\n",
    "  user write code\n",
    "  bot give code\n",
    "  last_user_message)\n",
    "  code['text'])\n",
    "  code,test =text\n",
    "\"\"\"\n",
    "     \n",
    "\n",
    "config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "     \n",
    "\n",
    "app = LLMRails(config,verbose=True)\n",
    "     \n",
    "\n",
    "app.register_action(chain3, name=\"chain3\")\n",
    "app.register_action(chain4, name=\"chain4\")\n",
    "app.register_action(parse2, name=\"parse2\")\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "new_message_hf = await app.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"帮我写一个归并排序算法\",\n",
    "}])\n",
    "new_message_hf\n",
    "     \n",
    "\n",
    "> Entering new TransformChain chain...\n",
    "\n",
    "> Finished chain.\n",
    "{'role': 'assistant',\n",
    " 'content': '好的 我会帮你写一个这样的算法\\n   File \"/content/code.py\", line 24    result.append(right_arr[right_                           ^SyntaxError: \\'[\\' was never closed'}\n",
    "\n",
    "print(new_message_hf['content'])\n",
    "     \n",
    "好的 我会帮你写一个这样的算法\n",
    "   File \"/content/code.py\", line 24    result.append(right_arr[right_                           ^SyntaxError: '[' was never closed\n",
    "\n",
    "!python code.py\n",
    "     \n",
    "  File \"/content/code.py\", line 24\n",
    "    result.append(right_arr[right_\n",
    "                           ^\n",
    "SyntaxError: '[' was never closed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5569492-8f63-4b5b-83b5-f52945e9a7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6e542-5016-4e4e-9563-0a2dd9523918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c74d4-e384-4f81-a934-d70cc7fbf8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90664dd-6e4e-4676-a272-b49ac65ee1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask capabilities\n",
    "  \"你能做什么?\"\n",
    "  \"你能帮助我什么\"\n",
    "  \"告诉我你的能力\"\n",
    "  \"聊聊你自己\"\n",
    "\n",
    "define bot inform capabilities\n",
    "  \"我是一只小小机器人 QE\"\n",
    "define flow\n",
    "  user ask capabilities\n",
    "  bot inform capabilities\n",
    "\n",
    "\"\"\"\n",
    "COLANG_CONFIG1 = \"\"\"\n",
    "define user express greeting\n",
    " \"hello\"\n",
    " \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    " \"Hello there, $name!\"\n",
    "\n",
    "define flow hello\n",
    " user express greeting\n",
    " if $first_time_user\n",
    "  bot express greeting\n",
    "  bot ask welfare\n",
    " else\n",
    "  bot expess welcome back\n",
    "\n",
    "# 我们可以根据用户的响应有不同的路径（我们可以使用 when建模）：\n",
    "define flow hello\n",
    " user express greeting\n",
    " bot express greeting\n",
    " bot ask welfare\n",
    "\n",
    " when user express happiness\n",
    " bot express happiness\n",
    " else when user express sadness\n",
    " bot express empathy\n",
    "#子流旨在由其他流/子流明确调用。可以使用 do 调用\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: chatglm2-6b\n",
    "\"\"\"\n",
    "\n",
    "def register_llm_provider(name, llm):\n",
    "    import nemoguardrails\n",
    "\n",
    "    \"\"\"\n",
    "    !pip install langchain\n",
    "    !pip install openai\n",
    "    !pip install google-search-results\n",
    "    \"\"\"\n",
    "    \"\"\"Register a llm_server provider.\n",
    "    注册\n",
    "    register_llm_provider(\"ai21\", ai21LLM)\n",
    "    register_llm_provider(\"hf_dolly\", HFDollyLLM)\n",
    "    register_llm_provider(\"chatglm2-6b\", chatLLM)\n",
    "    \"\"\"\n",
    "    nemoguardrails.llm.providers.register_llm_provider(name, llm)\n",
    "\n",
    "\n",
    "async def run(COLANG_CONFIG,YAML_CONFIG,prompt = '你能帮助我什么'):\n",
    "    from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "\n",
    "    config = RailsConfig.from_content(COLANG_CONFIG,YAML_CONFIG)\n",
    "    app = LLMRails(config)\n",
    "\n",
    "    new_message_hf = await app.generate_async(messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "# class ChatLLM(LLM):\n",
    "#     \"\"\"A HuggingFace llm_server.\"\"\"\n",
    "#     max_length = 2000\n",
    "#     temperature = 0.9\n",
    "#\n",
    "#\n",
    "#\n",
    "#     # 创建并初始化一个构造函数\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)#调用父类的构造函数来初始化这个类的实例\n",
    "#\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"chatglm2-6b\"\n",
    "#\n",
    "#     #在langchain中分别同步调用和异步调用自定义LLM。参考https://python.langchain.com/docs/modules/chains/how_to/custom_chain。\n",
    "#     def _call(self, prompt, stop) -> str:\n",
    "#         123\n",
    "#         return \"你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\"\n",
    "#\n",
    "#     async def _acall(self, prompt, stop) -> str:\n",
    "#         if stop is not None:\n",
    "#             raise ValueError(\"stop kwargs are not permitted.\")\n",
    "#         print(\"prompt:\",prompt)\n",
    "#         answer = [i for i in stream_chat(prompt,history=[],sequence_id=10234)][-1]\n",
    "#         print('answer:',answer)\n",
    "#         return answer\n",
    "\n",
    "# class HFDollyLLM(LLM):\n",
    "#     \"\"\"A HuggingFace llm_server.\"\"\"\n",
    "#     llm = None\n",
    "\n",
    "#     # 创建并初始化一个构造函数\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)  # 调用父类的构造函数来初始化这个类的实例\n",
    "\n",
    "#         # self.name = 'HF Dolly 3B'\n",
    "#         repo_id = \"databricks/dolly-v2-3b\"\n",
    "#         self.llm = HuggingFacePipeline.from_model_id(\n",
    "#             model_id=repo_id,\n",
    "#             device=0,\n",
    "#             task=\"text-generation\",\n",
    "#             model_kwargs={\"temperature\": 0.8, \"max_length\": 680})\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         # 以字符串形式返回大语言模型的类型\n",
    "#         return \"hf_dolly\"\n",
    "\n",
    "#     # 在langchain中分别同步调用和异步调用自定义LLM。参考https://python.langchain.com/docs/modules/chains/how_to/custom_chain。\n",
    "#     def _call(self, prompt, stop, run_manager) -> str:\n",
    "#         return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "#     async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "#         return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "# class Ai21LLM(LLM):\n",
    "#     \"\"\"A ai21 llm_server.\"\"\"\n",
    "#     llm = None\n",
    "#     # 创建并初始化一个构造函数\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs) #调用父类的构造函数来初始化这个类的实例\n",
    "#         self.llm = AI21()\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         # 以字符串形式返回大语言模型的类型\n",
    "#         return \"ai21\"\n",
    "#     #在langchain中分别同步调用和异步调用自定义LLM\n",
    "#     def _call(self, prompt, stop, run_manager) -> str:\n",
    "#         return self.llm._call(prompt, stop, run_manager)\n",
    "\n",
    "#     async def _acall(self, prompt, stop, run_manager) -> str:\n",
    "#         return await self.llm._acall(prompt, stop, run_manager)\n",
    "\n",
    "\n",
    "def main():\n",
    "    llm = OpenAI()\n",
    "    chat_model = ChatOpenAI()\n",
    "\n",
    "    chat_model.predict(\"hi!\")\n",
    "\n",
    "    from langchain.schema import HumanMessage\n",
    "\n",
    "    text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "    messages = [HumanMessage(content=text)]\n",
    "\n",
    "    llm.predict_messages(messages)\n",
    "\n",
    "    chat_model.predict_messages(messages)\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"\n",
    "    Parse the output of an llm_server call to a comma-separated list.\n",
    "    CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
    "    \"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an llm_server call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "\n",
    "def prompt_template(template=\"What is a good name for a company that makes {product}?\"):\n",
    "    \"\"\"\n",
    "    prompt.format(product=\"colorful socks\")\n",
    "    :param template:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "    return prompt\n",
    "\n",
    "def chat_prompt_template(template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",human_template = \"{text}\"):\n",
    "    \"\"\"\n",
    "    chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
    "    :param template:\n",
    "    :param human_template:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", template),\n",
    "        (\"human\", human_template),\n",
    "    ])\n",
    "\n",
    "def init_agent(tools,llm):\n",
    "    \"\"\"\n",
    "    agent.run(\n",
    "        \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    agent = initialize_agent(\n",
    "        tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# colang\n",
    "\n",
    "__doc__ = \"\"\"\n",
    "colang 核心概念\n",
    "Utterance  语音：来自用户或机器人的原始文本\n",
    "Message   消息：用户/机器人话语的规范形式（即结构化表示\n",
    "Event      事件：已经发生并与对话相关的事情，例如用户沉默，用户点击了什么，用户做了一个手势等。\n",
    "Action      操作：机器人可以调用的自定义代码；通常用于连接到第三方API\n",
    "Context    上下文：与对话相关的任何数据（即键值词典)\n",
    "Flow       流：一系列消息和事件，可能具有额外的分支逻辑。\n",
    "Rails       Rails：控制对话系统（又名机器人）行为的具体方法，例如不谈论政治，以特定方式响应某些用户请求，遵循预定义的对话路径，使用特定的语言风格，提取数据等。Colang中的铁路可以通过一个或多个流进行建模。\n",
    "\n",
    "关键词参考\n",
    "bot：在定义机器人消息（define bot ...）和在流程中使用（bot ...）时使用时使用\n",
    "break：打破一个while循环；\n",
    "continue：继续进行while循环的下一次迭代；循环的外部类似于python中的pass；\n",
    "create：创建一个新事件；\n",
    "define：用于定义用户/机器人消息和流；\n",
    "do：用于调用子流；\n",
    "else：if和when块；\n",
    "execute：用于执行行动；\n",
    "event：用于匹配事件；\n",
    "flow：用于定义流量（define flow）\n",
    "goto：转到指定的标签；\n",
    "if：用于典型的if块；\n",
    "include：用于包括另一种rails配置；\n",
    "label：在流程中标记标签；\n",
    "meta：提供有关流量的元信息；\n",
    "priority：设置流的优先级\n",
    "return：结束电流流；\n",
    "set：设置上下文变量的内容；\n",
    "subflow：用于定义子流（define subflow）\n",
    "user：在定义用户消息（define user ...）和在流中使用（user ...）时使用时使用\n",
    "while：典型的while循环，类似于python；\n",
    "when：基于事件流的分支。\n",
    "\n",
    "声明\n",
    "简单的陈述\n",
    "一个简单的语句由单个逻辑行组成。\n",
    "bot：机器人说了些什么。\n",
    "break：打破电流while循环。\n",
    "continue：再次转到当前循环的开头；如果没有循环，则没有影响。\n",
    "return：结束电流。\n",
    "execute：执行操作。\n",
    "do：调用子流。\n",
    "event：发生了一个事件。\n",
    "goto：转到特定标签。\n",
    "include：包括另一个轨道配置。\n",
    "label：定义一个标签。\n",
    "meta：定义流的元信息。\n",
    "set：设置上下文变量的值。\n",
    "user：用户说了些什么。\n",
    "\n",
    "复合声明\n",
    "复合语句包含（组）其他语句；\n",
    "define action：定义操作及其参数（用于文档目的）。\n",
    "define bot：定义一个机器人消息。\n",
    "define flow/define subflow：定义流/子流。\n",
    "parallel：并行流可以同时有多个并行实例；\n",
    "test：测试流程仅用于测试\n",
    "sample：样本流仅用于文档\n",
    "extension：扩展流可以中断“决策元素”上的其他流\n",
    "continuous：连续流动不能中断，也就是说，如果它们不能继续，它们将被中止。\n",
    "define user：定义用户消息的示例。\n",
    "else：if/的替代路径when\n",
    "if：条件分支。\n",
    "while：重复执行。\n",
    "when：基于事件的分支。\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
