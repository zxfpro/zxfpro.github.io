{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8beab5ad-26e3-4802-ae42-32e338c777fc",
   "metadata": {},
   "source": [
    "# API 编程的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555ba143-f2c1-4e77-94bb-627c1100f245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flask import Flask, Response, request\n",
    "from flask import stream_with_context, abort\n",
    "import json\n",
    "import asyncio\n",
    "import base64\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def set_proctitle(name=''):\n",
    "    # 设置进程名称为 my_process\n",
    "    setproctitle.setproctitle(name)\n",
    "    # 以下是你的其他代码\n",
    "\n",
    "def verbose_func(info,verbose=True):\n",
    "    if verbose:\n",
    "        print(info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e16e9f-364f-473c-9f6c-8c5458403ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class API_manager():\n",
    "    def __init__(self,**kwargs):\n",
    "        \"\"\"\n",
    "        kwargs 包括\n",
    "        \"\"\"\n",
    "        \n",
    "        self.app = Flask(__name__)\n",
    "        self.token = '57d3d17868206f5e181fd27d0cbdde89d9739b0538b27ddd'\n",
    "        self.open_guard_token = True # 是否开启token鉴权\n",
    "        self.db_list = []\n",
    "        self.answer = None\n",
    "        self.answer_dict = {}\n",
    "        self.function_kwargs = kwargs\n",
    "        \n",
    "    def guard_token(self,type='get'):\n",
    "        try:\n",
    "            if type == 'get':\n",
    "                token = request.args.get('token', default=None, type=None)\n",
    "            elif type == 'post':\n",
    "                data = request.get_data()\n",
    "                data_dict = json.loads(data)\n",
    "                token = data_dict.get('token')\n",
    "            else:\n",
    "                raise Exception(\"type error\")\n",
    "            assert token == self.token\n",
    "            return None\n",
    "        except:\n",
    "            if self.open_guard_token:\n",
    "                return self.response_info(f\"token未授权\", 500, '')\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    def get_data(self,type='post'):\n",
    "        if type== 'post':\n",
    "            data = request.get_data()\n",
    "            data_dict = json.loads(data)\n",
    "        return data_dict\n",
    "\n",
    "    def response_info(self,msg:str, code:int, data):\n",
    "        dicts = {\n",
    "            \"msg\": msg,\n",
    "            \"code\": code,\n",
    "            \"data\": data\n",
    "        }\n",
    "        return Response(json.dumps(dicts,ensure_ascii=False), mimetype='application/json')\n",
    "    def response_generate(self,generate):\n",
    "        #TODO\n",
    "\n",
    "        return Response(generate(), mimetype='text/plain')\n",
    "    \n",
    "    def start(self, server_name='0.0.0.0', server_port=9000,run=True,**kwargs):\n",
    "        \n",
    "        @self.app.route('/live', methods=['GET','POST'])\n",
    "        def live():\n",
    "            \"\"\"\n",
    "            判断服务是否存活\n",
    "            \"\"\"\n",
    "            if request.method == \"GET\":\n",
    "                db_name = request.args.get('db_name', default=None, type=None)\n",
    "                return self.response_info(\"success\", 200, '')\n",
    "                \n",
    "            if request.method == \"POST\":\n",
    "                data = request.get_data()\n",
    "                data_dict = json.loads(data)\n",
    "                return self.response_info(f\"success\", 200, data_dict)\n",
    "\n",
    "#         @self.app.route('/post_demo', methods=['POST'])\n",
    "#         def add_db_name_api():\n",
    "#             if request.method == \"POST\":  \n",
    "#                 # token 鉴权\n",
    "#                 end = self.guard_token('post')\n",
    "#                 if end:\n",
    "#                     return end\n",
    "                \n",
    "#                 # 数据断言\n",
    "#                 data_dict = self.get_data('post')\n",
    "#                 try:\n",
    "#                     assert data_dict['data']\n",
    "#                 except:\n",
    "#                     return self.response_info(\"未找到data字段 请传入\", 500, data_dict)\n",
    "                \n",
    "#                 # 函数执行\n",
    "#                 try:\n",
    "#                     function = lambda x: x\n",
    "#                     result = function(data_dict['data'])\n",
    "#                 except:\n",
    "#                     return self.response_info(\"服务器错误\", 500, '测试1')\n",
    "#                 return self.response_info(f\"成功接收传入数据{data_dict['data']}\", 200, result)\n",
    "\n",
    "#         @self.app.route('/get_demo', methods=['GET'])\n",
    "#         def get_db_list_api():\n",
    "#             if request.method == \"GET\":\n",
    "#                 # token 鉴权      \n",
    "#                 end = self.guard_token('get')\n",
    "#                 if end:\n",
    "#                     return end\n",
    "#                 # 数据断言\n",
    "#                 data = request.args.get('data', default=None, type=None)\n",
    "#                 # 函数执行\n",
    "#                 function = lambda x: x\n",
    "#                 result = function(data)\n",
    "                \n",
    "#                 return self.response_info(\"返回输入的数据\", 200, result)\n",
    "\n",
    "        if run:\n",
    "            self.app.run(host=server_name, port=server_port,**kwargs)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df000c5a-9426-4331-a09b-168ba39486df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class API_massage2(API_manager):\n",
    "    def start(self, server_name='0.0.0.0', server_port=9000,**kwargs):\n",
    "        super().start(run=False)\n",
    "        # custom code\n",
    "        \n",
    "        # custom code end\n",
    "        self.app.run(host=server_name, port=server_port,**kwargs)\n",
    "API_massage2().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d938027-af15-4036-85b9-1f896a8e8bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a892428-d605-4273-b9a3-fc7a553f36ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d93a86f-a53a-48d4-ac79-237e4449c9d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 2283\n"
     ]
    }
   ],
   "source": [
    "%%prompter 编写一个获取数据库信息的get请求\n",
    "\n",
    "@self.app.route('/get_demo', methods=['GET'])\n",
    "def get_db_list_api():\n",
    "    if request.method == \"GET\":\n",
    "        # token 鉴权      \n",
    "        end = self.guard_token('get')\n",
    "        if end:\n",
    "            return end\n",
    "        # 数据断言\n",
    "        data = request.args.get('data', default=None, type=None)\n",
    "        # 函数执行\n",
    "        function = lambda x: x\n",
    "        result = function(data)\n",
    "\n",
    "        return self.response_info(\"返回输入的数据\", 200, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae13ee7-2fa4-4f1b-88dd-6496124a0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@self.app.route('/get_demo', methods=['GET'])\n",
    "def get_db_list_api():\n",
    "    if request.method == \"GET\":\n",
    "        # token 鉴权      \n",
    "        end = self.guard_token('get')\n",
    "        if end:\n",
    "            return end\n",
    "        # 数据断言\n",
    "        data = request.args.get('data', default=None, type=None)\n",
    "        # 函数执行\n",
    "        function = lambda x: x\n",
    "        result = function(data)\n",
    "\n",
    "        return self.response_info(\"返回输入的数据\", 200, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc87b4-ddc4-4238-b943-64efa81e7b4a",
   "metadata": {},
   "source": [
    "### 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161a258-3410-45db-a6ad-4c398ad773a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client():\n",
    "\n",
    "    import requests\n",
    "    r = requests.get('http://127.0.0.1:9000/live')\n",
    "    return r.text\n",
    "def client2():\n",
    "    import requests\n",
    "    import json\n",
    "    k = {\"data\": \"datas\",\n",
    "        \"token\": \"57d3d17868206f5e181fd27d0cbdde89d9739b0538b27ddd\"}\n",
    "    k = json.dumps(k)\n",
    "    r = requests.post('http://127.0.0.1:9000/post_demo',data=k)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9512bb-a903-4d2d-aa52-63e3727e2c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a9aef-cb33-45cc-9442-58be684efeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21781bc9-ae76-4f40-bbf4-281270480c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644e403-5eda-4ada-9767-16f763d0efb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74222e-145c-47f0-8349-cb07ae185ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fe9732-efdb-4e76-ab15-3f16477fbc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flask import Flask, Response, request\n",
    "from flask import stream_with_context, abort\n",
    "import json\n",
    "import asyncio\n",
    "import base64\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def set_proctitle(name=''):\n",
    "    # 设置进程名称为 my_process\n",
    "    setproctitle.setproctitle(name)\n",
    "    # 以下是你的其他代码\n",
    "\n",
    "def verbose_func(info,verbose=True):\n",
    "    if verbose:\n",
    "        print(info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e38157-2c15-412d-a1fd-06df5425d6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class API_manager():\n",
    "    def __init__(self,**kwargs):\n",
    "        \"\"\"\n",
    "        kwargs 包括\n",
    "        \"\"\"\n",
    "        \n",
    "        self.app = Flask(__name__)\n",
    "        self.token = '57d3d17868206f5e181fd27d0cbdde89d9739b0538b27ddd'\n",
    "        self.open_guard_token = True # 是否开启token鉴权\n",
    "        self.db_list = []\n",
    "        self.answer = None\n",
    "        self.answer_dict = {}\n",
    "        self.function_kwargs = kwargs\n",
    "        \n",
    "    def guard_token(self,type='get'):\n",
    "        try:\n",
    "            if type == 'get':\n",
    "                token = request.args.get('token', default=None, type=None)\n",
    "            elif type == 'post':\n",
    "                data = request.get_data()\n",
    "                data_dict = json.loads(data)\n",
    "                token = data_dict.get('token')\n",
    "            else:\n",
    "                raise Exception(\"type error\")\n",
    "            assert token == self.token\n",
    "            return None\n",
    "        except:\n",
    "            if self.open_guard_token:\n",
    "                return self.response_info(f\"token未授权\", 500, '')\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    def get_data(self,type='post'):\n",
    "        if type== 'post':\n",
    "            data = request.get_data()\n",
    "            data_dict = json.loads(data)\n",
    "        return data_dict\n",
    "\n",
    "    def response_info(self,msg:str, code:int, data):\n",
    "        dicts = {\n",
    "            \"msg\": msg,\n",
    "            \"code\": code,\n",
    "            \"data\": data\n",
    "        }\n",
    "        return Response(json.dumps(dicts,ensure_ascii=False), mimetype='application/json')\n",
    "    def response_generate(self,generate):\n",
    "        #TODO\n",
    "\n",
    "        return Response(generate(), mimetype='text/plain')\n",
    "    \n",
    "    def start(self, server_name='0.0.0.0', server_port=9000,run=True,**kwargs):\n",
    "        \n",
    "        @self.app.route('/live', methods=['GET','POST'])\n",
    "        def live():\n",
    "            \"\"\"\n",
    "            判断服务是否存活\n",
    "            \"\"\"\n",
    "            if request.method == \"GET\":\n",
    "                db_name = request.args.get('db_name', default=None, type=None)\n",
    "                return self.response_info(\"success\", 200, '')\n",
    "                \n",
    "            if request.method == \"POST\":\n",
    "                data = request.get_data()\n",
    "                data_dict = json.loads(data)\n",
    "                return self.response_info(f\"success\", 200, data_dict)\n",
    "\n",
    "#         @self.app.route('/get_demo', methods=['GET'])\n",
    "#         def get_db_list_api():\n",
    "#             if request.method == \"GET\":\n",
    "#                 # token 鉴权      \n",
    "#                 end = self.guard_token('get')\n",
    "#                 if end:\n",
    "#                     return end\n",
    "#                 # 数据断言\n",
    "#                 data = request.args.get('data', default=None, type=None)\n",
    "#                 # 函数执行\n",
    "#                 function = lambda x: x\n",
    "#                 result = function(data)\n",
    "                \n",
    "#                 return self.response_info(\"返回输入的数据\", 200, result)\n",
    "\n",
    "#         if run:\n",
    "#             self.app.run(host=server_name, port=server_port,**kwargs)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dd6793-3cc5-4aff-8852-c8088ca338a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd51b0c-f863-4452-b981-e2082439ef1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import uvicorn\n",
    "import json\n",
    "import torch\n",
    "from sse_starlette.sse import ServerSentEvent, EventSourceResponse\n",
    "import logging\n",
    "import sys\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2124b36-274a-4bbd-a74b-39528512627b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zxftools_dev.project.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918dd6f5-ef66-45de-985c-d3ce4c6a13c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = Logger('./log.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650f9577-969a-4a8c-8274-66e7991118e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"/Users/zhaoxuefeng/Downloads/glm9b\"\n",
    "MAX_HISTORY = 21\n",
    "MAX_LENGTH = 8192\n",
    "TOP_P = 0.8\n",
    "TEMPERATURE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29b49780-ae71-4ff9-92e8-c0d75afee22d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:9003\n",
      " * Running on http://198.18.0.1:9003\n",
      "Press CTRL+C to quit\n",
      "[2024-06-07 11:21:31,108] ERROR in app: Exception on /v1/chat/completions [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 873, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 892, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1193, in make_response\n",
      "    raise TypeError(\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1188, in make_response\n",
      "    rv = self.response_class.force_type(\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/werkzeug/wrappers/response.py\", line 237, in force_type\n",
      "    response = Response(*run_wsgi_app(response, environ))\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/werkzeug/test.py\", line 1264, in run_wsgi_app\n",
      "    app_rv = app(environ, start_response)\n",
      "TypeError: EventSourceResponse.__call__() missing 1 required positional argument: 'send'\n",
      "The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a EventSourceResponse.\n",
      "127.0.0.1 - - [07/Jun/2024 11:21:31] \"POST /v1/chat/completions HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Hello!'}], 'model': 'glm4', 'stream': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-07 11:21:32,040] ERROR in app: Exception on /v1/chat/completions [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 873, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 892, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1193, in make_response\n",
      "    raise TypeError(\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1188, in make_response\n",
      "    rv = self.response_class.force_type(\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/werkzeug/wrappers/response.py\", line 237, in force_type\n",
      "    response = Response(*run_wsgi_app(response, environ))\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/werkzeug/test.py\", line 1264, in run_wsgi_app\n",
      "    app_rv = app(environ, start_response)\n",
      "TypeError: EventSourceResponse.__call__() missing 1 required positional argument: 'send'\n",
      "The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a EventSourceResponse.\n",
      "127.0.0.1 - - [07/Jun/2024 11:21:32] \"POST /v1/chat/completions HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Hello!'}], 'model': 'glm4', 'stream': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-07 11:21:33,812] ERROR in app: Exception on /v1/chat/completions [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 873, in full_dispatch_request\n",
      "    return self.finalize_request(rv)\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 892, in finalize_request\n",
      "    response = self.make_response(rv)\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1193, in make_response\n",
      "    raise TypeError(\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/flask/app.py\", line 1188, in make_response\n",
      "    rv = self.response_class.force_type(\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/werkzeug/wrappers/response.py\", line 237, in force_type\n",
      "    response = Response(*run_wsgi_app(response, environ))\n",
      "  File \"/Users/zhaoxuefeng/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/werkzeug/test.py\", line 1264, in run_wsgi_app\n",
      "    app_rv = app(environ, start_response)\n",
      "TypeError: EventSourceResponse.__call__() missing 1 required positional argument: 'send'\n",
      "The view function did not return a valid response. The return type must be a string, dict, list, tuple with headers or status, Response instance, or WSGI callable, but it was a EventSourceResponse.\n",
      "127.0.0.1 - - [07/Jun/2024 11:21:33] \"POST /v1/chat/completions HTTP/1.1\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Hello!'}], 'model': 'glm4', 'stream': True}\n"
     ]
    }
   ],
   "source": [
    "class API_massage2(API_manager):\n",
    "    def start(self, server_name='0.0.0.0', server_port=9000,**kwargs):\n",
    "        super().start(run=False)\n",
    "        # custom code\n",
    "        @self.app.route('/v1/chat/completions', methods=['POST'])\n",
    "        async def completions():\n",
    "            if request.method == \"POST\":  \n",
    "                def decorate(generator):\n",
    "                    for item in generator:\n",
    "                        yield ServerSentEvent(json.dumps(item, ensure_ascii=False))\n",
    "                    yield ServerSentEvent(data=\"[DONE]\")\n",
    "                    \n",
    "                def returns(dicts):\n",
    "                    return Response(json.dumps(dicts,ensure_ascii=False), mimetype='application/json')\n",
    "                    \n",
    "                try:\n",
    "                    data = request.get_data()\n",
    "                    arg_dict = json.loads(data)\n",
    "                    print(arg_dict)\n",
    "                    messages = arg_dict.get(\"messages\", [])\n",
    "                    text = messages[-1][\"content\"] if messages else \"\"\n",
    "                    history = []\n",
    "                    if len(messages)>1:\n",
    "                        history = messages[:-1]\n",
    "                        if len(history)>MAX_HISTORY:\n",
    "                            history = messages[0]+history[-MAX_HISTORY:]      \n",
    "\n",
    "                    top_p = arg_dict.get(\"top_p\",TOP_P)\n",
    "                    temperature = arg_dict.get(\"temperature\",TEMPERATURE)\n",
    "                    max_length = arg_dict.get(\"max_tokens\",MAX_LENGTH)\n",
    "\n",
    "                    if max_length < 1024:\n",
    "                        max_length = MAX_LENGTH\n",
    "\n",
    "                    if temperature == 0:\n",
    "                       temperature = 0.1 \n",
    "\n",
    "                    if arg_dict.get(\"stream\", False):\n",
    "                        # return EventSourceResponse(decorate(bot.stream(text, history,top_p = top_p,temperature = temperature,max_length = max_length)))\n",
    "                        # return EventSourceResponse(decorate(aa()))\n",
    "                    \n",
    "                    else:\n",
    "                        # response, history = bot.answer(text, history)\n",
    "                        print(text,history,'text_history')\n",
    "                        \n",
    "                        response='你来看看这个东东'\n",
    "                        \n",
    "                        return returns({\n",
    "                            \"model\": \"glm4\",\n",
    "                            \"id\": str(uuid.uuid4()),\n",
    "                            \"object\": \"chat.completion\",\n",
    "                            \"choices\": [\n",
    "                                {\n",
    "                                    \"index\": 0,\n",
    "                                    \"message\": {\n",
    "                                        \"role\": \"assistant\",\n",
    "                                        \"content\": response,\n",
    "                                        \"function_call\": None\n",
    "                                    },\n",
    "                                    \"finish_reason\": \"length\"\n",
    "                                }\n",
    "                            ]\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"error: {e}\")\n",
    "                    return returns({\n",
    "                        \"model\": \"glm4\",\n",
    "                        \"id\": str(uuid.uuid4()),\n",
    "                        \"object\": \"chat.completion\",\n",
    "                        \"choices\": [\n",
    "                            {\n",
    "                                \"index\": 0,\n",
    "                                \"message\": {\n",
    "                                    \"role\": \"assistant\",\n",
    "                                    \"content\": \"\",\n",
    "                                    \"function_call\": None\n",
    "                                },\n",
    "                                \"finish_reason\": \"length\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"msg\": str(e)\n",
    "                    })\n",
    "                \n",
    "        \n",
    "        # custom code end\n",
    "        self.app.run(host=server_name, port=server_port,**kwargs)\n",
    "API_massage2().start(server_port=9003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39beb37c-155a-4a9d-a7ec-a5d0f020b95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db19ad-cfd2-483f-995e-fc39729dae30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ae610-65df-44b9-8093-42eed7c54560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@self.app.route('/get_demo', methods=['GET'])\n",
    "def get_db_list_api():\n",
    "    if request.method == \"GET\":\n",
    "        # token 鉴权      \n",
    "        end = self.guard_token('get')\n",
    "        if end:\n",
    "            return end\n",
    "        # 数据断言\n",
    "        data = request.args.get('data', default=None, type=None)\n",
    "        # 函数执行\n",
    "        function = lambda x: x\n",
    "        result = function(data)\n",
    "\n",
    "        return self.response_info(\"返回输入的数据\", 200, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4251a-8d94-40ae-8d0c-4029c8723964",
   "metadata": {},
   "source": [
    "### 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06735dcc-4210-4f12-bf87-a54e04133364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client():\n",
    "\n",
    "    import requests\n",
    "    r = requests.get('http://127.0.0.1:9000/live')\n",
    "    return r.text\n",
    "def client2():\n",
    "    import requests\n",
    "    import json\n",
    "    k = {\"data\": \"datas\",\n",
    "        \"token\": \"57d3d17868206f5e181fd27d0cbdde89d9739b0538b27ddd\"}\n",
    "    k = json.dumps(k)\n",
    "    r = requests.post('http://127.0.0.1:9000/post_demo',data=k)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28c6f8-4824-46df-ad46-a0930707cfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd24a8-c8e8-4949-891b-1e554b0c59f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe86756-db7c-4419-a688-6d4c368623f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1cd9c-5a34-4b8b-ba1c-7daea8b3522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import uvicorn\n",
    "import json\n",
    "import torch\n",
    "from sse_starlette.sse import ServerSentEvent, EventSourceResponse\n",
    "import logging\n",
    "import sys\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01656ac-a269-4c5b-82d0-1564913aacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatGLM:\n",
    "    def __init__(self, model_name: str = MODEL_DIR) -> None:\n",
    "        logger.info(\"Start initialize model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = self._load_model(model_name)\n",
    "        self.model.eval()      \n",
    "        logger.info(\"Model initialization finished.\")\n",
    "    \n",
    "    def _load_model(self, model_name: str):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                                     # device_map='auto', \n",
    "                                                     # quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    "                                                    )\n",
    "        return model\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    \n",
    "    def answer(self, query: str, history: List[tuple],max_length: int = 81920, top_p: float = 0.9, temperature: float = 0.95) -> (str, List[tuple]):\n",
    "        response, history = self.model.chat(self.tokenizer, query, history=history, max_length=max_length, top_p=top_p, temperature=temperature)\n",
    "        return response, [list(h) for h in history]\n",
    "\n",
    "    def stream(self, query: str, history: List[tuple], max_length: int = 81920, top_p: float = 0.9, temperature: float = 0.95):\n",
    "        size = 0\n",
    "        response = \"\"\n",
    "        for response, history in self.model.stream_chat(self.tokenizer, query, history, max_length=max_length, top_p=top_p, temperature=temperature):\n",
    "        \n",
    "            this_response = response[size:]\n",
    "            size = len(response)\n",
    "            yield {\n",
    "                \"model\": \"glm4\",\n",
    "                \"id\": \"chatcmpl-\" + str(uuid.uuid4()),\n",
    "                \"object\": \"chat.completion.chunk\",\n",
    "                \"choices\": [\n",
    "                    {\n",
    "                        \"delta\": {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": this_response,\n",
    "                            \"function_call\": None\n",
    "                        },\n",
    "                        \"finish_reason\": \"length\",\n",
    "                        \"index\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }       \n",
    "        \n",
    "\n",
    "bot = ChatGLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83f1bd6a-2e4c-4b5a-800f-20636ff8ff00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c88b5976-097a-44cd-902e-6201d90c6864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aa():\n",
    "    a = [1]*10\n",
    "    b = [2]*10\n",
    "    for response,history in zip(a,b):\n",
    "\n",
    "        yield {\n",
    "            \"model\": \"glm4\",\n",
    "            \"id\": \"chatcmpl-\" + str(uuid.uuid4()),\n",
    "            \"object\": \"chat.completion.chunk\",\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"delta\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response,\n",
    "                        \"function_call\": None\n",
    "                    },\n",
    "                    \"finish_reason\": \"length\",\n",
    "                    \"index\": 0\n",
    "                }\n",
    "            ]\n",
    "        }       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab538975-bbc9-4df8-bb2c-6ebf5b1a0a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'glm4', 'id': 'chatcmpl-1f415934-1b14-4864-9618-7487c942503e', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-7e0d27e5-326e-483e-af10-366b9c094ded', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-c96ad307-f504-410a-be00-6728d2233dfb', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-02407b9e-fc77-43e0-83d5-7fd2ba2f374f', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-aef60c08-cdb8-4b1f-9a97-8bb1990ca445', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-68dbbfc2-8738-4c8a-b544-0a0fd65a6e1a', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-de9a9298-d6b1-48bf-936f-e8c35efc792a', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-6175fa76-b541-4c50-bdb9-83e69ad7d183', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-7c565d55-ce2f-4243-b4be-d795da170d06', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n",
      "{'model': 'glm4', 'id': 'chatcmpl-8355e6cc-a683-4103-933d-7ada55a8b11d', 'object': 'chat.completion.chunk', 'choices': [{'delta': {'role': 'assistant', 'content': 1, 'function_call': None}, 'finish_reason': 'length', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "for i in aa():\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bdedb-4976-4505-add4-c5cfb3bf2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream2():\n",
    "    for response, history in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb0de1-806b-4439-badd-0979dbfa9ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4dbd5-a30f-406e-a90a-3d6dff2cdb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6559f6-9c8d-4088-a866-dba8b8d6624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a4238-5e19-4d48-b3fd-fd057946e41c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455de517-1cc9-40bf-8a05-ea2c9764cd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flask import Flask, Response, request\n",
    "from flask import stream_with_context, abort\n",
    "import json\n",
    "import asyncio\n",
    "import base64\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def set_proctitle(name=''):\n",
    "    # 设置进程名称为 my_process\n",
    "    setproctitle.setproctitle(name)\n",
    "    # 以下是你的其他代码\n",
    "\n",
    "def verbose_func(info,verbose=True):\n",
    "    if verbose:\n",
    "        print(info)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1906a-655b-4215-a4ba-aa44dddb4529",
   "metadata": {},
   "source": [
    "### 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf21366-2518-44cd-8e07-69e79be15050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client():\n",
    "\n",
    "    import requests\n",
    "    r = requests.get('http://127.0.0.1:9000/live')\n",
    "    return r.text\n",
    "def client2():\n",
    "    import requests\n",
    "    import json\n",
    "    k = {\"data\": \"datas\",\n",
    "        \"token\": \"57d3d17868206f5e181fd27d0cbdde89d9739b0538b27ddd\"}\n",
    "    k = json.dumps(k)\n",
    "    r = requests.post('http://127.0.0.1:9000/post_demo',data=k)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e689fa6-2506-43d2-8484-dcd5d93efdd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "client = OpenAI(base_url='http://127.0.0.1:9003/v1/',api_key='sdfsdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dd09926-3ee2-4524-b3ef-08e8babdafed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "<!doctype html>\n<html lang=en>\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglm4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(completion.choices[0].message)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/py310_new/lib/python3.10/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: <!doctype html>\n<html lang=en>\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"glm4\",\n",
    "  stream = True,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a5de2b4-8324-4290-8ec3-ebc94c15cb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in completion:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
